{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d096648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage:\n",
      "Index                                       132\n",
      "PatientID                                  4788\n",
      "T1c_exponential_firstorder_10Percentile     608\n",
      "T1c_exponential_firstorder_90Percentile     608\n",
      "T1c_exponential_firstorder_Energy           608\n",
      "                                           ... \n",
      "EOR                                        3970\n",
      "Biopsy prior to imaging                    3885\n",
      "BraTS21 ID                                 4096\n",
      "BraTS21 Segmentation Cohort                3736\n",
      "BraTS21 MGMT Cohort                        3686\n",
      "Length: 20570, dtype: int64\n",
      "\n",
      "Memory usage after optimization:\n",
      "Index                                       132\n",
      "T1c_exponential_firstorder_10Percentile     304\n",
      "T1c_exponential_firstorder_90Percentile     304\n",
      "T1c_exponential_firstorder_Energy           608\n",
      "T1c_exponential_firstorder_Entropy          304\n",
      "                                           ... \n",
      "IDH                                        4361\n",
      "1-dead 0-alive                              608\n",
      "OS                                          304\n",
      "EOR                                        3970\n",
      "Biopsy prior to imaging                    3885\n",
      "Length: 20566, dtype: int64\n",
      "\n",
      "Features shape: (45, 20563)\n",
      "Target shape: (45,)\n",
      "Before SMOTE: 45 samples\n",
      "After SMOTE: 48 samples\n",
      "Class distribution after SMOTE:\n",
      "Survival_Category\n",
      "0    12\n",
      "1    12\n",
      "2    12\n",
      "3    12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Step 1: Load the data\n",
    "# df = pd.read_csv('radiomics_cleaned_merged.csv')  # Replace with your actual merged filename\n",
    "\n",
    "# # Check initial memory usage\n",
    "# print(f\"Initial memory usage:\\n{df.memory_usage(deep=True)}\\n\")\n",
    "\n",
    "# # Step 2: Drop unnecessary columns to reduce memory usage\n",
    "# df = df.drop(columns=[\n",
    "#     \"PatientID\", \"BraTS21 ID\", \"BraTS21 Segmentation Cohort\", \"BraTS21 MGMT Cohort\",\n",
    "# ], errors=\"ignore\")\n",
    "\n",
    "# # Step 3: Downcast numerical columns to save memory\n",
    "# for col in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "#     df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "# # Check memory usage again after dropping columns and downcasting\n",
    "# print(f\"Memory usage after optimization:\\n{df.memory_usage(deep=True)}\\n\")\n",
    "\n",
    "# # Step 4: Keep only patients who are dead (status = 1) and valid OS values\n",
    "# df = df[df[\"1-dead 0-alive\"] == 1]\n",
    "# df = df[df[\"OS\"].notna()]\n",
    "# df[\"OS\"] = pd.to_numeric(df[\"OS\"], errors=\"coerce\")\n",
    "# df = df[df[\"OS\"] > 0]\n",
    "\n",
    "# # Step 5: Perform quartile binning on OS\n",
    "# percentiles = np.percentile(df[\"OS\"], [25, 50, 75])\n",
    "# bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]\n",
    "# labels = [0, 1, 2, 3]\n",
    "# df[\"Survival_Category\"] = pd.cut(df[\"OS\"], bins=bins, labels=labels)\n",
    "\n",
    "# # Step 6: Separate features (X) and target (y)\n",
    "# X = df.drop(columns=[\"OS\", \"Survival_Category\", \"1-dead 0-alive\"], errors=\"ignore\")\n",
    "# y = df[\"Survival_Category\"]\n",
    "\n",
    "# # Step 7: Encode categorical variables\n",
    "# categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "# label_encoders = {}\n",
    "# for col in categorical_cols:\n",
    "#     le = LabelEncoder()\n",
    "#     X[col] = le.fit_transform(X[col].astype(str))\n",
    "#     label_encoders[col] = le\n",
    "\n",
    "# # Step 8: Impute missing values using median strategy\n",
    "# imputer = SimpleImputer(strategy=\"median\")\n",
    "# X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# # Step 9: Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# # Step 10: Check the shape of X and y\n",
    "# print(f\"Features shape: {X_scaled.shape}\")\n",
    "# print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# # Step 11: Apply SMOTE to handle class imbalance\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_smote, y_smote = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# # Check the number of samples before and after SMOTE\n",
    "# print(f\"Before SMOTE: {X_scaled.shape[0]} samples\")\n",
    "# print(f\"After SMOTE: {X_smote.shape[0]} samples\")\n",
    "# print(f\"Class distribution after SMOTE:\\n{pd.Series(y_smote).value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ed0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:34:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:35:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:35:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:35:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:35:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:36:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:36:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.38      0.50      0.42         5\n",
      "weighted avg       0.50      0.60      0.53         5\n",
      "\n",
      "\n",
      "Stacking Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.33      1.00      0.50         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.33      0.50      0.38         5\n",
      "weighted avg       0.47      0.60      0.50         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Splitting the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_scaled, y, test_size=0.1, stratify=y, random_state=42\n",
    "# )\n",
    "\n",
    "# # SMOTE to handle class imbalance\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Base models\n",
    "# rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "# xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "# logreg = LogisticRegression(max_iter=1000)\n",
    "# lda_clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "# # Voting and stacking classifiers\n",
    "# voting = VotingClassifier(estimators=[\n",
    "#     (\"rf\", rf), (\"xgb\", xgb_clf), (\"logreg\", logreg), (\"lda\", lda_clf)\n",
    "# ], voting=\"hard\")\n",
    "\n",
    "# stacking = StackingClassifier(\n",
    "#     estimators=[(\"rf\", rf), (\"xgb\", xgb_clf), (\"logreg\", logreg), (\"lda\", lda_clf)],\n",
    "#     final_estimator=RandomForestClassifier()\n",
    "# )\n",
    "\n",
    "# # Training the classifiers\n",
    "# voting.fit(X_train_resampled, y_train_resampled)\n",
    "# stacking.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Predicting on the test set\n",
    "# y_pred_voting = voting.predict(X_test)\n",
    "# y_pred_stacking = stacking.predict(X_test)\n",
    "\n",
    "# # Classification report for Voting Classifier\n",
    "# print(\"\\nVoting Classifier Report:\")\n",
    "# print(classification_report(y_test, y_pred_voting))\n",
    "\n",
    "# # Classification report for Stacking Classifier\n",
    "# print(\"\\nStacking Classifier Report:\")\n",
    "# print(classification_report(y_test, y_pred_stacking))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_resampled: (44, 20563)\n",
      "Shape of y_train_resampled: (44,)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Shape of X_train_resampled: {X_train_resampled.shape}\")\n",
    "# print(f\"Shape of y_train_resampled: {y_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE:\n",
      "Survival_Category\n",
      "0    11\n",
      "1    11\n",
      "2    11\n",
      "3    11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Checking the class distribution after SMOTE\n",
    "# print(f\"Class distribution after SMOTE:\\n{y_train_resampled.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d19ab2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4a7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PatientID', 'T1c_exponential_firstorder_10Percentile',\n",
      "       'T1c_exponential_firstorder_90Percentile',\n",
      "       'T1c_exponential_firstorder_Energy',\n",
      "       'T1c_exponential_firstorder_Entropy',\n",
      "       'T1c_exponential_firstorder_InterquartileRange',\n",
      "       'T1c_exponential_firstorder_Kurtosis',\n",
      "       'T1c_exponential_firstorder_Maximum',\n",
      "       'T1c_exponential_firstorder_MeanAbsoluteDeviation',\n",
      "       'T1c_exponential_firstorder_Mean',\n",
      "       ...\n",
      "       'MGMT index', '1p/19q', 'IDH', '1-dead 0-alive', 'OS', 'EOR',\n",
      "       'Biopsy prior to imaging', 'BraTS21 ID', 'BraTS21 Segmentation Cohort',\n",
      "       'BraTS21 MGMT Cohort'],\n",
      "      dtype='object', length=20569)\n",
      "Features shape: (76, 20568)\n",
      "Target shape: (76,)\n",
      "Class distribution before SMOTE:\n",
      "Survival_Category\n",
      "0    19\n",
      "1    19\n",
      "2    19\n",
      "3    19\n",
      "Name: count, dtype: int64\n",
      "Before SMOTE: 76 samples\n",
      "After SMOTE: 76 samples\n",
      "Class distribution after SMOTE:\n",
      "Survival_Category\n",
      "0    19\n",
      "1    19\n",
      "2    19\n",
      "3    19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv('radiomics_cleaned_merged.csv')  # Replace with your actual merged filename\n",
    "\n",
    "# Check available columns\n",
    "print(df.columns)\n",
    "\n",
    "# Step 5: Create the Survival_Category column if it doesn't exist\n",
    "if 'Survival_Category' not in df.columns:\n",
    "    percentiles = np.percentile(df[\"OS\"], [25, 50, 75])\n",
    "    bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]\n",
    "    labels = [0, 1, 2, 3]\n",
    "    df[\"Survival_Category\"] = pd.cut(df[\"OS\"], bins=bins, labels=labels)\n",
    "\n",
    "# Step 6: Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"OS\", \"1-dead 0-alive\"], errors=\"ignore\")\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Step 7: Encode categorical variables\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Step 8: Impute missing values using median strategy\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Step 9: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Step 10: Check the shape of X and y\n",
    "print(f\"Features shape: {X_scaled.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Check the class distribution before SMOTE\n",
    "print(f\"Class distribution before SMOTE:\\n{y.value_counts()}\")\n",
    "\n",
    "# Step 11: Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Step 12: Check the number of samples before and after SMOTE\n",
    "print(f\"Before SMOTE: {X_scaled.shape[0]} samples\")\n",
    "print(f\"After SMOTE: {X_smote.shape[0]} samples\")\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(f\"Class distribution after SMOTE:\\n{pd.Series(y_smote).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81993e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:15:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:15:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:16:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:16:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:16:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:16:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:16:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       0.67      1.00      0.80         2\n",
      "           2       0.50      0.50      0.50         2\n",
      "           3       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.79      0.75      0.74         8\n",
      "weighted avg       0.79      0.75      0.74         8\n",
      "\n",
      "\n",
      "Stacking Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the already balanced SMOTE data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_smote, y_smote, test_size=0.1, stratify=y_smote, random_state=42\n",
    ")\n",
    "\n",
    "# Define lighter base models to avoid memory issues\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=50,          # reduce number of trees\n",
    "    max_depth=3,              # smaller trees\n",
    "    learning_rate=0.1,        # conservative learning\n",
    "    subsample=0.8,            # use part of data per tree\n",
    "    colsample_bytree=0.8,     # use part of features\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Ensemble models\n",
    "voting = VotingClassifier(estimators=[\n",
    "    (\"rf\", rf), (\"xgb\", xgb_clf), (\"logreg\", logreg), (\"lda\", lda_clf)\n",
    "], voting=\"hard\")\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[(\"rf\", rf), (\"xgb\", xgb_clf), (\"logreg\", logreg), (\"lda\", lda_clf)],\n",
    "    final_estimator=LogisticRegression(max_iter=500)  # simpler meta-model\n",
    ")\n",
    "\n",
    "# Fit models\n",
    "voting.fit(X_train, y_train)\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_voting = voting.predict(X_test)\n",
    "y_pred_stacking = stacking.predict(X_test)\n",
    "\n",
    "# Reports\n",
    "print(\"\\nVoting Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_voting))\n",
    "\n",
    "print(\"\\nStacking Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6aadf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc55721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.33      0.50      0.40         2\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.38         8\n",
      "   macro avg       0.46      0.38      0.39         8\n",
      "weighted avg       0.46      0.38      0.39         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\nRandom Forest Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098935c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [16:16:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(\"\\nXGBoost Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59315f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.17      0.50      0.25         2\n",
      "\n",
      "    accuracy                           0.12         8\n",
      "   macro avg       0.04      0.12      0.06         8\n",
      "weighted avg       0.04      0.12      0.06         8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Devyansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"\\nLogistic Regression Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f06004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA\n",
    "lda_clf.fit(X_train, y_train)\n",
    "y_pred_lda = lda_clf.predict(X_test)\n",
    "print(\"\\nLDA Report:\")\n",
    "print(classification_report(y_test, y_pred_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1e001",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b44a2",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39dab9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244c219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-08 13:32:19,284] A new study created in memory with name: no-name-4b434489-5b5b-4294-9eca-a0e91cb45f2c\n",
      "[I 2025-05-08 13:32:19,589] Trial 0 finished with value: 0.28974358974358977 and parameters: {'n_estimators': 52, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.28974358974358977.\n",
      "[I 2025-05-08 13:32:20,211] Trial 1 finished with value: 0.3030769230769231 and parameters: {'n_estimators': 144, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.3030769230769231.\n",
      "[I 2025-05-08 13:32:20,524] Trial 2 finished with value: 0.32974358974358975 and parameters: {'n_estimators': 59, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.32974358974358975.\n",
      "[I 2025-05-08 13:32:21,045] Trial 3 finished with value: 0.3169230769230769 and parameters: {'n_estimators': 131, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.32974358974358975.\n",
      "[I 2025-05-08 13:32:21,419] Trial 4 finished with value: 0.34307692307692306 and parameters: {'n_estimators': 87, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.34307692307692306.\n",
      "[I 2025-05-08 13:32:21,744] Trial 5 finished with value: 0.3425641025641026 and parameters: {'n_estimators': 66, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.34307692307692306.\n",
      "[I 2025-05-08 13:32:22,096] Trial 6 finished with value: 0.3158974358974359 and parameters: {'n_estimators': 76, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.34307692307692306.\n",
      "[I 2025-05-08 13:32:22,601] Trial 7 finished with value: 0.27692307692307694 and parameters: {'n_estimators': 127, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.34307692307692306.\n",
      "[I 2025-05-08 13:32:23,066] Trial 8 finished with value: 0.3169230769230769 and parameters: {'n_estimators': 114, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.34307692307692306.\n",
      "[I 2025-05-08 13:32:23,502] Trial 9 finished with value: 0.35692307692307695 and parameters: {'n_estimators': 104, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:24,214] Trial 10 finished with value: 0.3564102564102564 and parameters: {'n_estimators': 191, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:24,987] Trial 11 finished with value: 0.3564102564102564 and parameters: {'n_estimators': 192, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:25,848] Trial 12 finished with value: 0.3169230769230769 and parameters: {'n_estimators': 191, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:26,543] Trial 13 finished with value: 0.31743589743589745 and parameters: {'n_estimators': 163, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:27,031] Trial 14 finished with value: 0.3169230769230769 and parameters: {'n_estimators': 95, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:27,743] Trial 15 finished with value: 0.31743589743589745 and parameters: {'n_estimators': 165, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:28,277] Trial 16 finished with value: 0.3035897435897436 and parameters: {'n_estimators': 107, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:28,927] Trial 17 finished with value: 0.3302564102564102 and parameters: {'n_estimators': 154, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:29,587] Trial 18 finished with value: 0.3030769230769231 and parameters: {'n_estimators': 179, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:30,117] Trial 19 finished with value: 0.3035897435897436 and parameters: {'n_estimators': 140, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:30,572] Trial 20 finished with value: 0.34307692307692306 and parameters: {'n_estimators': 107, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:31,305] Trial 21 finished with value: 0.3564102564102564 and parameters: {'n_estimators': 197, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:31,964] Trial 22 finished with value: 0.31743589743589745 and parameters: {'n_estimators': 180, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:32,626] Trial 23 finished with value: 0.3169230769230769 and parameters: {'n_estimators': 180, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:33,368] Trial 24 finished with value: 0.31641025641025644 and parameters: {'n_estimators': 199, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:33,982] Trial 25 finished with value: 0.31743589743589745 and parameters: {'n_estimators': 166, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:34,458] Trial 26 finished with value: 0.3035897435897436 and parameters: {'n_estimators': 117, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:34,862] Trial 27 finished with value: 0.31743589743589745 and parameters: {'n_estimators': 94, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:35,536] Trial 28 finished with value: 0.3169230769230769 and parameters: {'n_estimators': 188, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.35692307692307695.\n",
      "[I 2025-05-08 13:32:36,094] Trial 29 finished with value: 0.3041025641025641 and parameters: {'n_estimators': 143, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.35692307692307695.\n"
     ]
    }
   ],
   "source": [
    "def rf_objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    model = RandomForestClassifier(**params)\n",
    "    return cross_val_score(model, X_smote, y_smote, cv=cv, scoring=\"accuracy\").mean()\n",
    "\n",
    "rf_study = optuna.create_study(direction=\"maximize\")\n",
    "rf_study.optimize(rf_objective, n_trials=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2652eb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-08 13:32:36,103] A new study created in memory with name: no-name-9b2e03a3-0b80-47e3-83dc-f19bf8dadfd2\n",
      "[I 2025-05-08 13:32:47,108] Trial 0 finished with value: 0.8425641025641025 and parameters: {'n_estimators': 74, 'max_depth': 2, 'learning_rate': 0.16062846732697408, 'subsample': 0.6731672981908506, 'colsample_bytree': 0.691987854135004}. Best is trial 0 with value: 0.8425641025641025.\n",
      "[I 2025-05-08 13:33:02,763] Trial 1 finished with value: 0.8174358974358974 and parameters: {'n_estimators': 91, 'max_depth': 6, 'learning_rate': 0.13875722155695103, 'subsample': 0.6949464600938782, 'colsample_bytree': 0.7050992432260849}. Best is trial 0 with value: 0.8425641025641025.\n",
      "[I 2025-05-08 13:33:12,806] Trial 2 finished with value: 0.9743589743589745 and parameters: {'n_estimators': 75, 'max_depth': 2, 'learning_rate': 0.1951070594609566, 'subsample': 0.8232778367252738, 'colsample_bytree': 0.9420648881812127}. Best is trial 2 with value: 0.9743589743589745.\n",
      "[I 2025-05-08 13:33:26,607] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 90, 'max_depth': 3, 'learning_rate': 0.16429304367692707, 'subsample': 0.9706553374491685, 'colsample_bytree': 0.7009529393123899}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:34:04,152] Trial 4 finished with value: 0.8307692307692308 and parameters: {'n_estimators': 135, 'max_depth': 4, 'learning_rate': 0.013354612497024266, 'subsample': 0.6192129893812987, 'colsample_bytree': 0.6851451305589366}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:34:17,251] Trial 5 finished with value: 0.9871794871794872 and parameters: {'n_estimators': 97, 'max_depth': 2, 'learning_rate': 0.15983560101108685, 'subsample': 0.8532965455186254, 'colsample_bytree': 0.8817149526603227}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:34:27,550] Trial 6 finished with value: 0.8041025641025641 and parameters: {'n_estimators': 76, 'max_depth': 2, 'learning_rate': 0.1814181141866104, 'subsample': 0.6963375137986182, 'colsample_bytree': 0.6650496132557397}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:34:37,529] Trial 7 finished with value: 0.9738461538461539 and parameters: {'n_estimators': 77, 'max_depth': 2, 'learning_rate': 0.175708316691321, 'subsample': 0.8582755158139947, 'colsample_bytree': 0.6590783972661346}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:34:52,152] Trial 8 finished with value: 0.9353846153846154 and parameters: {'n_estimators': 114, 'max_depth': 5, 'learning_rate': 0.18406956001728642, 'subsample': 0.7656776021169156, 'colsample_bytree': 0.747042200696558}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:35:02,173] Trial 9 finished with value: 0.8964102564102564 and parameters: {'n_estimators': 51, 'max_depth': 4, 'learning_rate': 0.18081798963667567, 'subsample': 0.719240670002471, 'colsample_bytree': 0.8957840559811634}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:35:11,244] Trial 10 finished with value: 1.0 and parameters: {'n_estimators': 32, 'max_depth': 3, 'learning_rate': 0.09607981113680691, 'subsample': 0.9980279975798885, 'colsample_bytree': 0.8123029846412182}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:35:19,880] Trial 11 finished with value: 1.0 and parameters: {'n_estimators': 30, 'max_depth': 3, 'learning_rate': 0.09873798818840235, 'subsample': 0.9908966194902111, 'colsample_bytree': 0.8030643185873558}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:35:43,184] Trial 12 finished with value: 1.0 and parameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.08413044923526497, 'subsample': 0.9771912757808037, 'colsample_bytree': 0.8117731244735488}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:35:53,627] Trial 13 finished with value: 1.0 and parameters: {'n_estimators': 35, 'max_depth': 3, 'learning_rate': 0.0570704569971616, 'subsample': 0.9236134016670463, 'colsample_bytree': 0.7656064293215878}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:36:05,384] Trial 14 finished with value: 1.0 and parameters: {'n_estimators': 52, 'max_depth': 3, 'learning_rate': 0.12965087868381348, 'subsample': 0.9230891079367698, 'colsample_bytree': 0.6045409887815938}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:36:29,303] Trial 15 finished with value: 1.0 and parameters: {'n_estimators': 116, 'max_depth': 5, 'learning_rate': 0.06392680746400756, 'subsample': 0.9188237081843457, 'colsample_bytree': 0.8411859300999556}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:36:45,799] Trial 16 finished with value: 1.0 and parameters: {'n_estimators': 108, 'max_depth': 4, 'learning_rate': 0.12163672832332437, 'subsample': 0.9945484477814556, 'colsample_bytree': 0.7507959912208376}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:37:01,341] Trial 17 finished with value: 0.9743589743589745 and parameters: {'n_estimators': 54, 'max_depth': 3, 'learning_rate': 0.0320074183398701, 'subsample': 0.9384147356888722, 'colsample_bytree': 0.6107645291475234}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:37:15,053] Trial 18 finished with value: 1.0 and parameters: {'n_estimators': 61, 'max_depth': 5, 'learning_rate': 0.10839559516236288, 'subsample': 0.8780239875176724, 'colsample_bytree': 0.9955966921915556}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:37:32,631] Trial 19 finished with value: 0.9610256410256411 and parameters: {'n_estimators': 135, 'max_depth': 4, 'learning_rate': 0.1463279843361056, 'subsample': 0.7914261348431627, 'colsample_bytree': 0.8500178759612386}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:37:43,147] Trial 20 finished with value: 1.0 and parameters: {'n_estimators': 38, 'max_depth': 3, 'learning_rate': 0.09141217496119901, 'subsample': 0.9584724839123271, 'colsample_bytree': 0.7240421676214259}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:37:52,321] Trial 21 finished with value: 1.0 and parameters: {'n_estimators': 33, 'max_depth': 3, 'learning_rate': 0.1024071716582866, 'subsample': 0.999129845891245, 'colsample_bytree': 0.7838213366559901}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:38:04,159] Trial 22 finished with value: 1.0 and parameters: {'n_estimators': 43, 'max_depth': 3, 'learning_rate': 0.06954062150816218, 'subsample': 0.8873372902317765, 'colsample_bytree': 0.8136219662581191}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:38:17,869] Trial 23 finished with value: 1.0 and parameters: {'n_estimators': 65, 'max_depth': 3, 'learning_rate': 0.10980133521560403, 'subsample': 0.9647437718818048, 'colsample_bytree': 0.8016909722483444}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:38:26,913] Trial 24 finished with value: 1.0 and parameters: {'n_estimators': 31, 'max_depth': 4, 'learning_rate': 0.08513237112490162, 'subsample': 0.9572534668899748, 'colsample_bytree': 0.8505078520479835}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:38:37,564] Trial 25 finished with value: 1.0 and parameters: {'n_estimators': 44, 'max_depth': 3, 'learning_rate': 0.12341322885398731, 'subsample': 0.8956940443716555, 'colsample_bytree': 0.9019521716532091}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:38:50,336] Trial 26 finished with value: 1.0 and parameters: {'n_estimators': 64, 'max_depth': 2, 'learning_rate': 0.0444568566893228, 'subsample': 0.9999743212342254, 'colsample_bytree': 0.7793360843084002}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:39:09,203] Trial 27 finished with value: 1.0 and parameters: {'n_estimators': 103, 'max_depth': 4, 'learning_rate': 0.09476901630670004, 'subsample': 0.9437619972908139, 'colsample_bytree': 0.7271598679739683}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:39:27,410] Trial 28 finished with value: 1.0 and parameters: {'n_estimators': 87, 'max_depth': 3, 'learning_rate': 0.07397695987403705, 'subsample': 0.9789186616022967, 'colsample_bytree': 0.8269612713642363}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-05-08 13:39:35,415] Trial 29 finished with value: 0.8964102564102564 and parameters: {'n_estimators': 44, 'max_depth': 2, 'learning_rate': 0.15436152188370128, 'subsample': 0.6086431019561107, 'colsample_bytree': 0.870238367362393}. Best is trial 3 with value: 1.0.\n"
     ]
    }
   ],
   "source": [
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 30, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"use_label_encoder\": False,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    return cross_val_score(model, X_smote, y_smote, cv=cv, scoring=\"accuracy\").mean()\n",
    "\n",
    "xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "xgb_study.optimize(xgb_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b638289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-08 13:39:35,425] A new study created in memory with name: no-name-7d15728b-a0d9-41fa-b670-f2b5685ab1ee\n",
      "[I 2025-05-08 13:39:37,579] Trial 0 finished with value: 0.27743589743589747 and parameters: {'C': 8.35402817287357, 'solver': 'liblinear'}. Best is trial 0 with value: 0.27743589743589747.\n",
      "[I 2025-05-08 13:40:21,084] Trial 1 finished with value: 0.2764102564102564 and parameters: {'C': 8.263847423092232, 'solver': 'saga'}. Best is trial 0 with value: 0.27743589743589747.\n",
      "[I 2025-05-08 13:40:21,440] Trial 2 finished with value: 0.2764102564102564 and parameters: {'C': 3.505023580832271, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.27743589743589747.\n",
      "[I 2025-05-08 13:40:22,905] Trial 3 finished with value: 0.2907692307692308 and parameters: {'C': 0.05069314455007318, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:40:23,880] Trial 4 finished with value: 0.2764102564102564 and parameters: {'C': 0.04912300897911432, 'solver': 'lbfgs'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:41:07,057] Trial 5 finished with value: 0.2764102564102564 and parameters: {'C': 1.5967555321281988, 'solver': 'saga'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:41:37,094] Trial 6 finished with value: 0.2764102564102564 and parameters: {'C': 0.06484090947430657, 'solver': 'saga'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:20,529] Trial 7 finished with value: 0.2764102564102564 and parameters: {'C': 0.45689694501521516, 'solver': 'saga'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:20,947] Trial 8 finished with value: 0.26307692307692304 and parameters: {'C': 1.0645011202886887, 'solver': 'lbfgs'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:22,433] Trial 9 finished with value: 0.2907692307692308 and parameters: {'C': 0.06908979163885592, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:23,656] Trial 10 finished with value: 0.27743589743589747 and parameters: {'C': 0.011468202181453382, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:25,135] Trial 11 finished with value: 0.2907692307692308 and parameters: {'C': 0.06492155851515852, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:26,678] Trial 12 finished with value: 0.2907692307692308 and parameters: {'C': 0.17439618815664393, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:28,368] Trial 13 finished with value: 0.27743589743589747 and parameters: {'C': 0.012638492901782337, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:29,960] Trial 14 finished with value: 0.2907692307692308 and parameters: {'C': 0.21888176132428566, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:31,255] Trial 15 finished with value: 0.2907692307692308 and parameters: {'C': 0.03133089540434235, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:32,775] Trial 16 finished with value: 0.2907692307692308 and parameters: {'C': 0.12019084283576825, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:34,054] Trial 17 finished with value: 0.2907692307692308 and parameters: {'C': 0.027624048323788147, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:35,770] Trial 18 finished with value: 0.2907692307692308 and parameters: {'C': 0.34499142477864153, 'solver': 'liblinear'}. Best is trial 3 with value: 0.2907692307692308.\n",
      "[I 2025-05-08 13:42:36,629] Trial 19 finished with value: 0.2764102564102564 and parameters: {'C': 0.023500936503279793, 'solver': 'lbfgs'}. Best is trial 3 with value: 0.2907692307692308.\n"
     ]
    }
   ],
   "source": [
    "def logreg_objective(trial):\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 0.01, 10.0, log=True),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"liblinear\", \"saga\"]),\n",
    "        \"max_iter\": 1000,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    model = LogisticRegression(**params)\n",
    "    return cross_val_score(model, X_smote, y_smote, cv=cv, scoring=\"accuracy\").mean()\n",
    "\n",
    "logreg_study = optuna.create_study(direction=\"maximize\")\n",
    "logreg_study.optimize(logreg_objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6962702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lda_objective(trial):\n",
    "#     solver = trial.suggest_categorical(\"solver\", [\"svd\", \"lsqr\", \"eigen\"])\n",
    "#     shrinkage = None\n",
    "#     if solver in [\"lsqr\", \"eigen\"]:\n",
    "#         shrinkage = trial.suggest_categorical(\"shrinkage\", [\"auto\", None])\n",
    "\n",
    "#     model = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)\n",
    "#     return cross_val_score(model, X_smote, y_smote, cv=cv, scoring=\"accuracy\").mean()\n",
    "\n",
    "# lda_study = optuna.create_study(direction=\"maximize\")\n",
    "# lda_study.optimize(lda_objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080824ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate best models\n",
    "best_rf = RandomForestClassifier(**rf_study.best_params, n_jobs=-1)\n",
    "best_xgb = xgb.XGBClassifier(**xgb_study.best_params)\n",
    "best_logreg = LogisticRegression(**logreg_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e37ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
