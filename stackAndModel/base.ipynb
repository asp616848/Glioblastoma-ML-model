{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_13188\\362985526.py:17: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy with LDA: 0.94\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        14\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       0.92      0.92      0.92        13\n",
      "           4       1.00      1.00      1.00        13\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.92      0.92      0.92        13\n",
      "           7       1.00      0.92      0.96        13\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.94       129\n",
      "   macro avg       0.94      0.94      0.94       129\n",
      "weighted avg       0.94      0.94      0.94       129\n",
      "\n",
      "Stacking Model Accuracy with LDA: 0.93\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        14\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       0.92      0.92      0.92        13\n",
      "           4       0.87      1.00      0.93        13\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.92      0.85      0.88        13\n",
      "           7       1.00      0.92      0.96        13\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.93       129\n",
      "   macro avg       0.93      0.93      0.93       129\n",
      "weighted avg       0.93      0.93      0.93       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [10,20,30,40,50,60,70,80,90])\n",
    "bins = [0, percentiles[0], percentiles[1],  percentiles[2],  percentiles[3],  percentiles[4],  percentiles[5],  percentiles[6],  percentiles[7],  percentiles[8], np.inf]\n",
    "labels = [0, 1, 2,3,4,5,6,7,8,9]\n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA for dimensionality reduction\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)  # Adjust components based on the number of classes - 1\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance Classes with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define Base Models\n",
    "rf_clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Voting Classifier (Soft Voting for Probabilistic Averaging)\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg), (\"LDA\", lda_clf)],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg), (\"LDA\", lda_clf)],\n",
    "    final_estimator=RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# Train the models\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "report_ensemble = classification_report(y_test, y_pred_ensemble)\n",
    "report_stacking = classification_report(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Ensemble Model Accuracy with LDA: {accuracy_ensemble:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_ensemble)\n",
    "\n",
    "print(f\"Stacking Model Accuracy with LDA: {accuracy_stacking:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_11644\\2492321000.py:17: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy with LDA: 0.84\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        14\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       0.92      0.92      0.92        13\n",
      "           3       0.50      0.92      0.65        13\n",
      "           4       0.93      1.00      0.96        13\n",
      "           5       0.85      0.92      0.88        12\n",
      "           6       0.00      0.00      0.00        13\n",
      "           7       1.00      0.92      0.96        13\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.84       129\n",
      "   macro avg       0.79      0.84      0.81       129\n",
      "weighted avg       0.79      0.84      0.81       129\n",
      "\n",
      "Stacking Model Accuracy with LDA: 0.84\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        14\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       0.92      0.92      0.92        13\n",
      "           3       0.50      0.92      0.65        13\n",
      "           4       0.87      1.00      0.93        13\n",
      "           5       0.92      0.92      0.92        12\n",
      "           6       0.00      0.00      0.00        13\n",
      "           7       1.00      0.92      0.96        13\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.84       129\n",
      "   macro avg       0.79      0.84      0.81       129\n",
      "weighted avg       0.79      0.84      0.81       129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [10,20,30,40,50,60,70,80,90])\n",
    "bins = [0, percentiles[0], percentiles[1],  percentiles[2],  percentiles[3],  percentiles[4],  percentiles[5],  percentiles[6],  percentiles[7],  percentiles[8], np.inf]\n",
    "labels = [0, 1, 2,3,4,5,6,7,8,9]\n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA for dimensionality reduction\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)  # Adjust components based on the number of classes - 1\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance Classes with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define Base Models\n",
    "rf_clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Voting Classifier (Soft Voting for Probabilistic Averaging)\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg), (\"LDA\", lda_clf)],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[ (\"LogReg\", log_reg),(\"LDA\", lda_clf)],\n",
    "    final_estimator=RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# Train the models\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "report_ensemble = classification_report(y_test, y_pred_ensemble)\n",
    "report_stacking = classification_report(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Ensemble Model Accuracy with LDA: {accuracy_ensemble:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_ensemble)\n",
    "\n",
    "print(f\"Stacking Model Accuracy with LDA: {accuracy_stacking:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (Ensemble): 0.84\n",
      "Balanced Accuracy (Stacking): 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_acc_ensemble = balanced_accuracy_score(y_test, y_pred_ensemble)\n",
    "balanced_acc_stacking = balanced_accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Balanced Accuracy (Ensemble): {balanced_acc_ensemble:.2f}\")\n",
    "print(f\"Balanced Accuracy (Stacking): {balanced_acc_stacking:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index (Ensemble): 0.86\n",
      "Concordance Index (Stacking): 0.86\n"
     ]
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "# Get original survival times for y_test samples\n",
    "# Make sure the test set index aligns with the original dataframe\n",
    "true_survival_times = y_test.copy()\n",
    "true_survival_times = df.loc[y_test.index, \"Survival_from_surgery_days_UPDATED\"]\n",
    "\n",
    "# Define bin midpoints (same binning as earlier)\n",
    "bin_midpoints = [(bins[i] + bins[i+1]) / 2 for i in range(len(bins) - 1)]\n",
    "\n",
    "# Map predicted categories to survival time estimates\n",
    "pred_survival_ensemble = [bin_midpoints[int(label)] for label in y_pred_ensemble]\n",
    "pred_survival_stacking = [bin_midpoints[int(label)] for label in y_pred_stacking]\n",
    "\n",
    "# Compute Concordance Index\n",
    "c_index_ensemble = concordance_index(true_survival_times, pred_survival_ensemble)\n",
    "c_index_stacking = concordance_index(true_survival_times, pred_survival_stacking)\n",
    "\n",
    "print(f\"Concordance Index (Ensemble): {c_index_ensemble:.2f}\")\n",
    "print(f\"Concordance Index (Stacking): {c_index_stacking:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the same XGBoost classifier used in the ensemble separately on the original scaled features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# (Not on LDA-transformed, because SHAP needs feature-level interpretability)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mxgb_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# SHAP explainer for tree-based models\u001b[39;00m\n\u001b[0;32m      9\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(xgb_clf)\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1597\u001b[0m )\n\u001b[1;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit the same XGBoost classifier used in the ensemble separately on the original scaled features\n",
    "# (Not on LDA-transformed, because SHAP needs feature-level interpretability)\n",
    "xgb_clf.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "# SHAP explainer for tree-based models\n",
    "explainer = shap.Explainer(xgb_clf)\n",
    "shap_values = explainer(X_test_orig)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, pd.DataFrame(X_test_orig, columns=X.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
