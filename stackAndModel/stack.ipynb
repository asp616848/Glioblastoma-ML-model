{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved at D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# File paths\n",
    "clinical_file = r\"D:\\mlpr data\\Glioblastoma-ML-model\\UPENN-GBM_clinical_info_v2.1.csv\"\n",
    "radiomics_folder = r\"D:\\mlpr data\\radiomic_features_CaPTk\"\n",
    "\n",
    "# Load clinical data\n",
    "clinical_df = pd.read_csv(clinical_file)\n",
    "clinical_df.rename(columns={\"ID\": \"PatientID\"}, inplace=True)  # Standardizing ID column name\n",
    "\n",
    "# Load all radiomic CSVs and merge horizontally on PatientID\n",
    "radiomic_files = glob(os.path.join(radiomics_folder, \"*.csv\"))\n",
    "\n",
    "# Initialize empty dataframe for radiomics\n",
    "radiomics_df = pd.DataFrame()\n",
    "\n",
    "for file in radiomic_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.rename(columns={\"SubjectID\": \"PatientID\"}, inplace=True)  # Standardizing ID column name\n",
    "    \n",
    "    # Merge radiomics files horizontally\n",
    "    if radiomics_df.empty:\n",
    "        radiomics_df = df\n",
    "    else:\n",
    "        radiomics_df = pd.merge(radiomics_df, df, on=\"PatientID\", how=\"outer\")\n",
    "\n",
    "# Merge clinical data with radiomics data\n",
    "merged_df = pd.merge(clinical_df, radiomics_df, on=\"PatientID\", how=\"outer\")\n",
    "\n",
    "# Save final merged dataset\n",
    "output_file = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved at {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_31244\\2365178571.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (forcing errors='coerce' turns non-numeric values into NaN)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\"])  # Drop ID and target\n",
    "y = df[\"Survival_from_surgery_days_UPDATED\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoder for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 9516)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)  # Use all cores\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 325.74\n",
      "R² Score: 0.0541\n",
      "Root Mean Squared Error (RMSE): 466.49\n",
      "Explained Variance Score: 0.0661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Root Mean Squared Error\n",
    "evs = explained_variance_score(y_test, y_pred)  # Explained Variance Score\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Explained Variance Score: {evs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_31244\\2159967707.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.39      0.43        33\n",
      "           1       0.30      0.19      0.23        32\n",
      "           2       0.24      0.31      0.27        32\n",
      "           3       0.25      0.31      0.28        32\n",
      "\n",
      "    accuracy                           0.30       129\n",
      "   macro avg       0.32      0.30      0.30       129\n",
      "weighted avg       0.32      0.30      0.30       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (handling non-numeric values)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# **Convert Survival Days into Categories (Example Binning)**\n",
    "# You can modify bins as per your requirement\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [25,50, 75])  \n",
    "bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]  \n",
    "labels = [0, 1, 2, 3]  # Adjust as needed\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_31244\\571684290.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41        33\n",
      "           1       0.31      0.25      0.28        32\n",
      "           2       0.29      0.28      0.29        32\n",
      "           3       0.28      0.31      0.29        32\n",
      "\n",
      "    accuracy                           0.32       129\n",
      "   macro avg       0.32      0.32      0.32       129\n",
      "weighted avg       0.32      0.32      0.32       129\n",
      "\n",
      "Number of PCA components used: 182\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (handling non-numeric values)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [25,50, 75])  \n",
    "bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]  \n",
    "labels = [0, 1, 2, 3]  # Adjust as needed\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.9)  # Retain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Print the number of PCA components\n",
    "print(f\"Number of PCA components used: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_46752\\2457060133.py:15: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.52\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        43\n",
      "           1       0.38      0.36      0.37        42\n",
      "           2       0.70      0.59      0.64        44\n",
      "\n",
      "    accuracy                           0.52       129\n",
      "   macro avg       0.53      0.52      0.52       129\n",
      "weighted avg       0.53      0.52      0.52       129\n",
      "\n",
      "Stacking Model Accuracy: 0.49\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.56      0.53        43\n",
      "           1       0.39      0.36      0.38        42\n",
      "           2       0.56      0.55      0.55        44\n",
      "\n",
      "    accuracy                           0.49       129\n",
      "   macro avg       0.48      0.49      0.48       129\n",
      "weighted avg       0.49      0.49      0.49       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [33, 66])\n",
    "bins = [0, percentiles[0], percentiles[1], np.inf]\n",
    "labels = [0, 1, 2]  \n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature Selection using RandomForest\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=150, random_state=42), max_features=100)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance Classes with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define Base Models\n",
    "rf_clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Voting Classifier (Soft Voting for Probabilistic Averaging)\n",
    "ensemble_model_1 = VotingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg)],\n",
    "    voting=\"soft\", weights=[1.5, 1, 2.5]  # Example of weight assignment\n",
    ")\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg)],\n",
    "    final_estimator=RandomForestClassifier()\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Train Ensemble Model\n",
    "ensemble_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ensemble_model_1.predict(X_test)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "report = classification_report(y_test, y_pred)\n",
    "report_stacking = classification_report(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "print(f\"Stacking Model Accuracy: {accuracy_stacking:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.1 or less. Got NumPy 2.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE, ADASYN, BorderlineSMOTE\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\shap\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explanation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cohorts, Explanation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m other\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\shap\\_explanation.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mslicer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Alias, Obj, Slicer\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_clustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hclust_ordering\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_general\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpChain\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\shap\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_clustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     delta_minimization_order,\n\u001b[0;32m      3\u001b[0m     hclust,\n\u001b[0;32m      4\u001b[0m     hclust_ordering,\n\u001b[0;32m      5\u001b[0m     partition_tree,\n\u001b[0;32m      6\u001b[0m     partition_tree_shuffle,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_general\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     OpChain,\n\u001b[0;32m     10\u001b[0m     approximate_interactions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     suppress_stderr,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_masked_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaskedModel, make_masks\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\shap\\utils\\_clustering.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m njit\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_progress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_progress\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numba\\__init__.py:59\u001b[0m\n\u001b[0;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[1;32mc:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numba\\__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.1 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 2.1 or less. Got NumPy 2.2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [33, 66])\n",
    "bins = [0, percentiles[0], percentiles[1], np.inf]\n",
    "labels = [0, 1, 2]  \n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"].astype(int)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature Selection using SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=50)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance Classes with BorderlineSMOTE\n",
    "resampler = BorderlineSMOTE(random_state=42)\n",
    "X_train, y_train = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define Base Models\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "log_reg = LogisticRegression(C=0.5, max_iter=1000)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=4)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_clf, param_grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "xgb_clf = grid_search.best_estimator_\n",
    "\n",
    "# Voting Classifier (Soft Voting)\n",
    "ensemble_model_2 = VotingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg), (\"GB\", gb_clf)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "ensemble_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ensemble_model_2.predict(X_test)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate Models\n",
    "print(f\"Ensemble Model Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Stacking Model Accuracy: {accuracy_score(y_test, y_pred_stacking):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacking))\n",
    "\n",
    "# Feature Importance Analysis\n",
    "def analyze_feature_importance(model, X):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "    plt.title(\"Feature Importance via SHAP Values\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_feature_importance(xgb_clf, X_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_27304\\83410862.py:16: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(file_path)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [   2    3   69   74   80  103  104  105  107  108  110  213  218  224\n",
      "  247  248  249  251  252  254  357  362  368  391  392  393  501  506\n",
      "  512  535  536  537  539  540  542  645  650  656  679  680  681  683\n",
      "  684  686  789  794  800  823  824  825  933  938  944  967  968  969\n",
      "  971  972  974 1077 1082 1088 1111 1112 1113 1115 1116 1118 1221 1226\n",
      " 1232 1255 1256 1257 1365 1370 1376 1399 1400 1401 1403 1404 1406 1509\n",
      " 1514 1520 1543 1544 1545 1547 1548 1550 1653 1658 1664 1687 1688 1689\n",
      " 1797 1802 1808 1831 1832 1833 1835 1836 1838 1941 1946 1952 1975 1976\n",
      " 1977 1979 1980 1982 2085 2090 2096 2119 2120 2121 2229 2234 2240 2263\n",
      " 2264 2265 2267 2268 2270 2373 2378 2384 2407 2408 2409 2411 2412 2414\n",
      " 2517 2522 2528 2551 2552 2553 2661 2666 2672 2695 2696 2697 2699 2700\n",
      " 2702 2805 2810 2816 2839 2840 2841 2843 2844 2846 2949 2954 2960 2983\n",
      " 2984 2985 3093 3098 3104 3127 3128 3129 3237 3242 3248 3271 3272 3273\n",
      " 3275 3276 3278 3381 3386 3392 3415 3416 3417 3525 3530 3536 3559 3560\n",
      " 3561 3669 3674 3680 3703 3704 3705 3707 3708 3710 3813 3818 3824 3847\n",
      " 3848 3849 3957 3962 3968 3991 3992 3993 4101 4106 4112 4135 4136 4137\n",
      " 4139 4140 4142 4245 4250 4256 4279 4280 4281 4389 4394 4400 4423 4424\n",
      " 4425 4533 4538 4544 4567 4568 4569 4571 4572 4574 4677 4682 4688 4711\n",
      " 4712 4713 4821 4826 4832 4855 4856 4857 4859 4860 4862 4965 4970 4976\n",
      " 4999 5000 5001 5003 5004 5006 5109 5114 5120 5143 5144 5145 5147 5148\n",
      " 5150 5253 5258 5264 5287 5288 5289 5291 5292 5294 5397 5402 5408 5431\n",
      " 5432 5433 5435 5436 5438 5541 5546 5552 5575 5576 5577 5579 5580 5582\n",
      " 5685 5690 5696 5719 5720 5721 5723 5724 5726 5829 5834 5840 5863 5864\n",
      " 5865 5867 5868 5870 5973 5978 5984 6007 6008 6009 6011 6012 6014 6117\n",
      " 6122 6128 6151 6152 6153 6155 6156 6158 6261 6266 6272 6295 6296 6297\n",
      " 6299 6300 6302 6405 6410 6416 6439 6440 6441 6443 6444 6446 6549 6554\n",
      " 6560 6583 6584 6585 6587 6588 6590 6693 6698 6704 6727 6728 6729 6731\n",
      " 6732 6734 6837 6842 6848 6871 6872 6873 6875 6876 6878 6981 6986 6992\n",
      " 7015 7016 7017 7019 7020 7022 7125 7130 7136 7159 7160 7161 7163 7164\n",
      " 7166 7269 7274 7280 7303 7304 7305 7307 7308 7310 7413 7418 7424 7447\n",
      " 7448 7449 7451 7452 7454 7557 7562 7568 7591 7592 7593 7595 7596 7598\n",
      " 7701 7706 7712 7735 7736 7737 7739 7740 7742 7845 7850 7856 7879 7880\n",
      " 7881 7883 7884 7886 7989 7994 8000 8023 8024 8025 8027 8028 8030 8133\n",
      " 8138 8144 8167 8168 8169 8171 8172 8174 8277 8282 8288 8311 8312 8313\n",
      " 8315 8316 8318 8421 8426 8432 8455 8456 8457 8459 8460 8462 8565 8570\n",
      " 8576 8599 8600 8601 8603 8604 8606 8709 8714 8720 8743 8744 8745 8747\n",
      " 8748 8750 8853 8858 8864 8887 8888 8889 8891 8892 8894 8997 9002 9008\n",
      " 9031 9032 9033 9035 9036 9038 9141 9146 9152 9175 9176 9177 9179 9180\n",
      " 9182 9285 9290 9296 9319 9320 9321 9323 9324 9326 9429 9434 9440 9463\n",
      " 9464 9465 9467 9468 9470] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[33  5  5]\n",
      " [17  7 18]\n",
      " [ 5 10 29]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.77      0.67        43\n",
      "           1       0.32      0.17      0.22        42\n",
      "           2       0.56      0.66      0.60        44\n",
      "\n",
      "    accuracy                           0.53       129\n",
      "   macro avg       0.49      0.53      0.50       129\n",
      "weighted avg       0.49      0.53      0.50       129\n",
      "\n",
      "\n",
      "Cross-Validation Scores: [0.47619048 0.4952381  0.47619048 0.52380952 0.52380952]\n",
      "Mean CV Score: 0.4990476190476191\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "\n",
    "class AdvancedEnsembleClassifier:\n",
    "    def __init__(self, file_path):\n",
    "        # Load data\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Convert target column to numeric\n",
    "        self.df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(\n",
    "            self.df[\"Survival_from_surgery_days_UPDATED\"], \n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # Drop rows where target variable is NaN\n",
    "        self.df = self.df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "        # Percentile-Based Binning with adjustment\n",
    "        percentiles = np.percentile(self.df[\"Survival_from_surgery_days_UPDATED\"], [33,66])\n",
    "        bins = [0, percentiles[0], percentiles[1], np.inf]\n",
    "        labels = [0, 1, 2]  \n",
    "\n",
    "        self.df[\"Survival_Category\"] = pd.cut(\n",
    "            self.df[\"Survival_from_surgery_days_UPDATED\"], \n",
    "            bins=bins, \n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = self.df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "        y = self.df[\"Survival_Category\"].astype(int)\n",
    "\n",
    "        # Encode categorical columns\n",
    "        categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "        self.label_encoders = {}\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "\n",
    "        # Fill missing numeric values with median\n",
    "        X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "        # Standardize numeric features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        # Feature Selection\n",
    "        self.selector = SelectKBest(score_func=f_classif, k=50)\n",
    "        X_selected = self.selector.fit_transform(X_scaled, y)\n",
    "\n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X_selected, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "\n",
    "        # Apply SMOTE for class balancing\n",
    "        smote = SMOTE(random_state=42)\n",
    "        self.X_train, self.y_train = smote.fit_resample(self.X_train, self.y_train)\n",
    "    \n",
    "    def create_models(self):\n",
    "        # Base models with improved parameters\n",
    "        rf_clf = RandomForestClassifier(\n",
    "            n_estimators=300, \n",
    "            max_depth=10, \n",
    "            min_samples_split=5, \n",
    "            min_samples_leaf=2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        xgb_clf = xgb.XGBClassifier(\n",
    "            n_estimators=250, \n",
    "            learning_rate=0.05, \n",
    "            max_depth=7, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        svm_clf = SVC(\n",
    "            kernel='rbf', \n",
    "            probability=True, \n",
    "            C=1.0, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Stacking Classifier with multiple base models\n",
    "        stacking_clf = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_clf),\n",
    "                ('xgb', xgb_clf),\n",
    "                ('svm', svm_clf)\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(max_iter=1000),\n",
    "            cv=5\n",
    "        )\n",
    "        \n",
    "        return stacking_clf\n",
    "    \n",
    "    def train_and_evaluate(self):\n",
    "        # Create and train the model\n",
    "        model = self.create_models()\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        # Detailed evaluation\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(self.y_test, y_pred))\n",
    "        \n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5)\n",
    "        print(\"\\nCross-Validation Scores:\", cv_scores)\n",
    "        print(\"Mean CV Score:\", cv_scores.mean())\n",
    "        \n",
    "        return model\n",
    "\n",
    "def main():\n",
    "    file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "    ensemble = AdvancedEnsembleClassifier(file_path)\n",
    "    model = ensemble.train_and_evaluate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
