{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved at D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# File paths\n",
    "clinical_file = r\"D:\\mlpr data\\Glioblastoma-ML-model\\UPENN-GBM_clinical_info_v2.1.csv\"\n",
    "radiomics_folder = r\"D:\\mlpr data\\radiomic_features_CaPTk\"\n",
    "\n",
    "# Load clinical data\n",
    "clinical_df = pd.read_csv(clinical_file)\n",
    "clinical_df.rename(columns={\"ID\": \"PatientID\"}, inplace=True)  # Standardizing ID column name\n",
    "\n",
    "# Load all radiomic CSVs and merge horizontally on PatientID\n",
    "radiomic_files = glob(os.path.join(radiomics_folder, \"*.csv\"))\n",
    "\n",
    "# Initialize empty dataframe for radiomics\n",
    "radiomics_df = pd.DataFrame()\n",
    "\n",
    "for file in radiomic_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.rename(columns={\"SubjectID\": \"PatientID\"}, inplace=True)  # Standardizing ID column name\n",
    "    \n",
    "    # Merge radiomics files horizontally\n",
    "    if radiomics_df.empty:\n",
    "        radiomics_df = df\n",
    "    else:\n",
    "        radiomics_df = pd.merge(radiomics_df, df, on=\"PatientID\", how=\"outer\")\n",
    "\n",
    "# Merge clinical data with radiomics data\n",
    "merged_df = pd.merge(clinical_df, radiomics_df, on=\"PatientID\", how=\"outer\")\n",
    "\n",
    "# Save final merged dataset\n",
    "output_file = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved at {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_18804\\2365178571.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (forcing errors='coerce' turns non-numeric values into NaN)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\"])  # Drop ID and target\n",
    "y = df[\"Survival_from_surgery_days_UPDATED\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoder for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 9516)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)  # Use all cores\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 342.39\n",
      "R² Score: 0.0764\n",
      "Root Mean Squared Error (RMSE): 475.31\n",
      "Explained Variance Score: 0.0794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Root Mean Squared Error\n",
    "evs = explained_variance_score(y_test, y_pred)  # Explained Variance Score\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Explained Variance Score: {evs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_18804\\2159967707.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.39      0.43        33\n",
      "           1       0.30      0.19      0.23        32\n",
      "           2       0.24      0.31      0.27        32\n",
      "           3       0.25      0.31      0.28        32\n",
      "\n",
      "    accuracy                           0.30       129\n",
      "   macro avg       0.32      0.30      0.30       129\n",
      "weighted avg       0.32      0.30      0.30       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (handling non-numeric values)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# **Convert Survival Days into Categories (Example Binning)**\n",
    "# You can modify bins as per your requirement\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [25,50, 75])  \n",
    "bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]  \n",
    "labels = [0, 1, 2, 3]  # Adjust as needed\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_18804\\571684290.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41        33\n",
      "           1       0.31      0.25      0.28        32\n",
      "           2       0.29      0.28      0.29        32\n",
      "           3       0.28      0.31      0.29        32\n",
      "\n",
      "    accuracy                           0.32       129\n",
      "   macro avg       0.32      0.32      0.32       129\n",
      "weighted avg       0.32      0.32      0.32       129\n",
      "\n",
      "Number of PCA components used: 182\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric (handling non-numeric values)\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [25,50, 75])  \n",
    "bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]  \n",
    "labels = [0, 1, 2, 3]  # Adjust as needed\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le  # Store encoders for future use\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")  # Ensure all values are numeric\n",
    "X = X.fillna(X.median())  # Replace NaNs with median\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.9)  # Retain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Print the number of PCA components\n",
    "print(f\"Number of PCA components used: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_19192\\3966034821.py:15: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.53\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        43\n",
      "           1       0.40      0.33      0.36        42\n",
      "           2       0.67      0.64      0.65        44\n",
      "\n",
      "    accuracy                           0.53       129\n",
      "   macro avg       0.52      0.52      0.52       129\n",
      "weighted avg       0.52      0.53      0.52       129\n",
      "\n",
      "Stacking Model Accuracy: 0.44\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.53      0.51        43\n",
      "           1       0.36      0.36      0.36        42\n",
      "           2       0.49      0.43      0.46        44\n",
      "\n",
      "    accuracy                           0.44       129\n",
      "   macro avg       0.44      0.44      0.44       129\n",
      "weighted avg       0.44      0.44      0.44       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [33, 66])\n",
    "bins = [0, percentiles[0], percentiles[1], np.inf]\n",
    "labels = [0, 1, 2]  \n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature Selection using RandomForest\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=150, random_state=42), max_features=100)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance Classes with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define Base Models\n",
    "rf_clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Voting Classifier (Soft Voting for Probabilistic Averaging)\n",
    "ensemble_model_1 = VotingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg)],\n",
    "    voting=\"soft\"  # Example of weight assignment\n",
    ")\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg)],\n",
    "    final_estimator=RandomForestClassifier()\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Train Ensemble Model\n",
    "ensemble_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ensemble_model_1.predict(X_test)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "report = classification_report(y_test, y_pred)\n",
    "report_stacking = classification_report(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "print(f\"Stacking Model Accuracy: {accuracy_stacking:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_18804\\3205822300.py:16: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles for Binning: [234.   506.52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [   2    3   69   74   80  103  104  105  107  108  110  213  218  224\n",
      "  247  248  249  251  252  254  357  362  368  391  392  393  501  506\n",
      "  512  535  536  537  539  540  542  645  650  656  679  680  681  683\n",
      "  684  686  789  794  800  823  824  825  933  938  944  967  968  969\n",
      "  971  972  974 1077 1082 1088 1111 1112 1113 1115 1116 1118 1221 1226\n",
      " 1232 1255 1256 1257 1365 1370 1376 1399 1400 1401 1403 1404 1406 1509\n",
      " 1514 1520 1543 1544 1545 1547 1548 1550 1653 1658 1664 1687 1688 1689\n",
      " 1797 1802 1808 1831 1832 1833 1835 1836 1838 1941 1946 1952 1975 1976\n",
      " 1977 1979 1980 1982 2085 2090 2096 2119 2120 2121 2229 2234 2240 2263\n",
      " 2264 2265 2267 2268 2270 2373 2378 2384 2407 2408 2409 2411 2412 2414\n",
      " 2517 2522 2528 2551 2552 2553 2661 2666 2672 2695 2696 2697 2699 2700\n",
      " 2702 2805 2810 2816 2839 2840 2841 2843 2844 2846 2949 2954 2960 2983\n",
      " 2984 2985 3093 3098 3104 3127 3128 3129 3237 3242 3248 3271 3272 3273\n",
      " 3275 3276 3278 3381 3386 3392 3415 3416 3417 3525 3530 3536 3559 3560\n",
      " 3561 3669 3674 3680 3703 3704 3705 3707 3708 3710 3813 3818 3824 3847\n",
      " 3848 3849 3957 3962 3968 3991 3992 3993 4101 4106 4112 4135 4136 4137\n",
      " 4139 4140 4142 4245 4250 4256 4279 4280 4281 4389 4394 4400 4423 4424\n",
      " 4425 4533 4538 4544 4567 4568 4569 4571 4572 4574 4677 4682 4688 4711\n",
      " 4712 4713 4821 4826 4832 4855 4856 4857 4859 4860 4862 4965 4970 4976\n",
      " 4999 5000 5001 5003 5004 5006 5109 5114 5120 5143 5144 5145 5147 5148\n",
      " 5150 5253 5258 5264 5287 5288 5289 5291 5292 5294 5397 5402 5408 5431\n",
      " 5432 5433 5435 5436 5438 5541 5546 5552 5575 5576 5577 5579 5580 5582\n",
      " 5685 5690 5696 5719 5720 5721 5723 5724 5726 5829 5834 5840 5863 5864\n",
      " 5865 5867 5868 5870 5973 5978 5984 6007 6008 6009 6011 6012 6014 6117\n",
      " 6122 6128 6151 6152 6153 6155 6156 6158 6261 6266 6272 6295 6296 6297\n",
      " 6299 6300 6302 6405 6410 6416 6439 6440 6441 6443 6444 6446 6549 6554\n",
      " 6560 6583 6584 6585 6587 6588 6590 6693 6698 6704 6727 6728 6729 6731\n",
      " 6732 6734 6837 6842 6848 6871 6872 6873 6875 6876 6878 6981 6986 6992\n",
      " 7015 7016 7017 7019 7020 7022 7125 7130 7136 7159 7160 7161 7163 7164\n",
      " 7166 7269 7274 7280 7303 7304 7305 7307 7308 7310 7413 7418 7424 7447\n",
      " 7448 7449 7451 7452 7454 7557 7562 7568 7591 7592 7593 7595 7596 7598\n",
      " 7701 7706 7712 7735 7736 7737 7739 7740 7742 7845 7850 7856 7879 7880\n",
      " 7881 7883 7884 7886 7989 7994 8000 8023 8024 8025 8027 8028 8030 8133\n",
      " 8138 8144 8167 8168 8169 8171 8172 8174 8277 8282 8288 8311 8312 8313\n",
      " 8315 8316 8318 8421 8426 8432 8455 8456 8457 8459 8460 8462 8565 8570\n",
      " 8576 8599 8600 8601 8603 8604 8606 8709 8714 8720 8743 8744 8745 8747\n",
      " 8748 8750 8853 8858 8864 8887 8888 8889 8891 8892 8894 8997 9002 9008\n",
      " 9031 9032 9033 9035 9036 9038 9141 9146 9152 9175 9176 9177 9179 9180\n",
      " 9182 9285 9290 9296 9319 9320 9321 9323 9324 9326 9429 9434 9440 9463\n",
      " 9464 9465 9467 9468 9470] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[31  7  5]\n",
      " [18  6 18]\n",
      " [ 5 11 28]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.72      0.64        43\n",
      "           1       0.25      0.14      0.18        42\n",
      "           2       0.55      0.64      0.59        44\n",
      "\n",
      "    accuracy                           0.50       129\n",
      "   macro avg       0.46      0.50      0.47       129\n",
      "weighted avg       0.46      0.50      0.47       129\n",
      "\n",
      "\n",
      "Cross-Validation Scores: [0.46666667 0.4952381  0.48571429 0.4952381  0.53333333]\n",
      "Mean CV Score: 0.49523809523809526\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "\n",
    "class AdvancedEnsembleClassifier:\n",
    "    def __init__(self, file_path):\n",
    "        # Load data\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Convert target column to numeric\n",
    "        self.df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(\n",
    "            self.df[\"Survival_from_surgery_days_UPDATED\"], \n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # Drop rows where target variable is NaN\n",
    "        self.df = self.df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "        # Percentile-Based Binning with adjustment\n",
    "        percentiles = np.percentile(self.df[\"Survival_from_surgery_days_UPDATED\"], [33,66])\n",
    "        bins = [0, percentiles[0], percentiles[1], np.inf]\n",
    "        print(\"Percentiles for Binning:\", percentiles)\n",
    "        labels = [0, 1, 2]  \n",
    "\n",
    "        self.df[\"Survival_Category\"] = pd.cut(\n",
    "            self.df[\"Survival_from_surgery_days_UPDATED\"], \n",
    "            bins=bins, \n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = self.df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "        y = self.df[\"Survival_Category\"].astype(int)\n",
    "\n",
    "        # Encode categorical columns\n",
    "        categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "        self.label_encoders = {}\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "\n",
    "        # Fill missing numeric values with median\n",
    "        X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "        # Standardize numeric features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        # Feature Selection\n",
    "        self.selector = SelectKBest(score_func=f_classif, k=50)\n",
    "        X_selected = self.selector.fit_transform(X_scaled, y)\n",
    "\n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X_selected, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "\n",
    "        # Apply SMOTE for class balancing\n",
    "        smote = SMOTE(random_state=42)\n",
    "        self.X_train, self.y_train = smote.fit_resample(self.X_train, self.y_train)\n",
    "    \n",
    "    def create_models(self):\n",
    "        # Base models with improved parameters\n",
    "        rf_clf = RandomForestClassifier(\n",
    "            n_estimators=300, \n",
    "            max_depth=10, \n",
    "            min_samples_split=5, \n",
    "            min_samples_leaf=2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        xgb_clf = xgb.XGBClassifier(\n",
    "            n_estimators=250, \n",
    "            learning_rate=0.05, \n",
    "            max_depth=40, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        svm_clf = SVC(\n",
    "            kernel='rbf', \n",
    "            probability=True, \n",
    "            C=1.0, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Stacking Classifier with multiple base models\n",
    "        stacking_clf = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_clf),\n",
    "                ('xgb', xgb_clf),\n",
    "                ('svm', svm_clf)\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(max_iter=1000),\n",
    "            cv=5\n",
    "        )\n",
    "        \n",
    "        return stacking_clf\n",
    "    \n",
    "    def train_and_evaluate(self):\n",
    "        # Create and train the model\n",
    "        model = self.create_models()\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        # Detailed evaluation\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(self.y_test, y_pred))\n",
    "        \n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5)\n",
    "        print(\"\\nCross-Validation Scores:\", cv_scores)\n",
    "        print(\"Mean CV Score:\", cv_scores.mean())\n",
    "        \n",
    "        return model\n",
    "\n",
    "def main():\n",
    "    file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "    ensemble = AdvancedEnsembleClassifier(file_path)\n",
    "    model = ensemble.train_and_evaluate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_19192\\770886090.py:16: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:34:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:34:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:35:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:35:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:35:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:35:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:35:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble (Voting) Accuracy: 0.54\n",
      "Voting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.63      0.56        43\n",
      "           1       0.43      0.36      0.39        42\n",
      "           2       0.70      0.64      0.67        44\n",
      "\n",
      "    accuracy                           0.54       129\n",
      "   macro avg       0.54      0.54      0.54       129\n",
      "weighted avg       0.54      0.54      0.54       129\n",
      "\n",
      "\n",
      "Stacking Accuracy: 0.48\n",
      "Stacking Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.58      0.55        43\n",
      "           1       0.36      0.31      0.33        42\n",
      "           2       0.53      0.55      0.54        44\n",
      "\n",
      "    accuracy                           0.48       129\n",
      "   macro avg       0.47      0.48      0.47       129\n",
      "weighted avg       0.47      0.48      0.48       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Target processing\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Binning the target\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [33, 66])\n",
    "bins = [0, percentiles[0], percentiles[1], np.inf]\n",
    "labels = [0, 1, 2]\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill and scale\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=150, random_state=42), max_features=100)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Split and balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define base models\n",
    "rf_clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "svm_clf = SVC(probability=True, kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "\n",
    "# Voting Classifier (soft voting)\n",
    "ensemble_model_2 = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomForest\", rf_clf),\n",
    "        (\"XGBoost\", xgb_clf),\n",
    "        (\"LogReg\", log_reg),\n",
    "        (\"SVM\", svm_clf)\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf_2 = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomForest\", rf_clf),\n",
    "        (\"XGBoost\", xgb_clf),\n",
    "        (\"LogReg\", log_reg),\n",
    "        (\"SVM\", svm_clf)\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "# Train models\n",
    "ensemble_model_2.fit(X_train, y_train)\n",
    "stacking_clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "y_pred = ensemble_model_2.predict(X_test)\n",
    "y_pred_stacking = stacking_clf_2.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "report_stacking = classification_report(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"\\nEnsemble (Voting) Accuracy: {accuracy:.2f}\")\n",
    "print(\"Voting Classification Report:\\n\", report)\n",
    "\n",
    "print(f\"\\nStacking Accuracy: {accuracy_stacking:.2f}\")\n",
    "print(\"Stacking Classification Report:\\n\", report_stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asp61\\AppData\\Local\\Temp\\ipykernel_19192\\2758369117.py:19: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.57\n",
      "Classification Report (Ensemble):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59        43\n",
      "           1       0.57      0.31      0.40        42\n",
      "           2       0.66      0.66      0.66        44\n",
      "\n",
      "    accuracy                           0.57       129\n",
      "   macro avg       0.57      0.56      0.55       129\n",
      "weighted avg       0.58      0.57      0.55       129\n",
      "\n",
      "Stacking Model Accuracy: 0.44\n",
      "Classification Report (Stacking):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.63      0.53        43\n",
      "           1       0.31      0.29      0.30        42\n",
      "           2       0.58      0.41      0.48        44\n",
      "\n",
      "    accuracy                           0.44       129\n",
      "   macro avg       0.45      0.44      0.44       129\n",
      "weighted avg       0.45      0.44      0.44       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "# Replace with your actual file path\n",
    "file_path =  r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning - fixed the bins issue\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [33, 66])\n",
    "bins = [0, percentiles[0], percentiles[1], np.inf]  # Corrected bins definition\n",
    "labels = [0, 1, 2]  \n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Train-test split (80:20) - Do this before scaling to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a preprocessing pipeline with scaling and feature selection\n",
    "preprocessor = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selector', SelectFromModel(\n",
    "        GradientBoostingClassifier(n_estimators=100, random_state=42), \n",
    "        max_features=50)  # Reduced from 100 to 50 features\n",
    "    )\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)  # Specify k_neighbors for better results\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "# Define Base Models with tuned hyperparameters\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=150, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=5, \n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    C=0.5,\n",
    "    solver='liblinear',\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=7,\n",
    "    weights='distance'\n",
    ")\n",
    "\n",
    "svm_clf = SVC(\n",
    "    C=1.0,\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Voting Classifier with adjusted weights\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomForest\", rf_clf), \n",
    "        (\"XGBoost\", xgb_clf), \n",
    "        (\"LogReg\", log_reg),\n",
    "        (\"KNN\", knn_clf),\n",
    "        (\"SVM\", svm_clf)\n",
    "    ],\n",
    "    voting=\"soft\", \n",
    "    weights=[2, 2, 1, 1, 1.5]  # Adjusted weights\n",
    ")\n",
    "\n",
    "# Stacking Classifier with a different meta-learner\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomForest\", rf_clf), \n",
    "        (\"XGBoost\", xgb_clf), \n",
    "        (\"LogReg\", log_reg),\n",
    "        (\"KNN\", knn_clf)\n",
    "    ],\n",
    "    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    cv=5  # 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# Train models\n",
    "ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_processed)\n",
    "y_pred_stacking = stacking_clf.predict(X_test_processed)\n",
    "\n",
    "# Evaluate models\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Ensemble Model Accuracy: {accuracy_ensemble:.2f}\")\n",
    "print(\"Classification Report (Ensemble):\\n\", classification_report(y_test, y_pred_ensemble))\n",
    "\n",
    "print(f\"Stacking Model Accuracy: {accuracy_stacking:.2f}\")\n",
    "print(\"Classification Report (Stacking):\\n\", classification_report(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asp61\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-09 13:42:42,025] A new study created in memory with name: no-name-db42d963-5cb5-437f-b4d4-c1dab7ff1308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Ensemble Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-09 13:42:43,621] Trial 0 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 195, 'rf_max_depth': 9, 'rf_min_samples_split': 3, 'xgb_n_estimators': 81, 'xgb_learning_rate': 0.012624668075432121, 'xgb_max_depth': 10, 'xgb_subsample': 0.943592368284729, 'xgb_colsample_bytree': 0.8578348706209307, 'log_reg_C': 0.03392751113421898, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:45,176] Trial 1 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 280, 'rf_max_depth': 12, 'rf_min_samples_split': 14, 'xgb_n_estimators': 83, 'xgb_learning_rate': 0.10716866082486512, 'xgb_max_depth': 4, 'xgb_subsample': 0.6476011172100687, 'xgb_colsample_bytree': 0.8064298718394076, 'log_reg_C': 0.5350465072659588, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:45,995] Trial 2 finished with value: 0.9446291615207899 and parameters: {'rf_n_estimators': 55, 'rf_max_depth': 11, 'rf_min_samples_split': 4, 'xgb_n_estimators': 120, 'xgb_learning_rate': 0.05799137436036437, 'xgb_max_depth': 9, 'xgb_subsample': 0.7652222039142667, 'xgb_colsample_bytree': 0.8447215728292252, 'log_reg_C': 7.393795312643763, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:47,846] Trial 3 finished with value: 0.9465391149758562 and parameters: {'rf_n_estimators': 210, 'rf_max_depth': 13, 'rf_min_samples_split': 4, 'xgb_n_estimators': 217, 'xgb_learning_rate': 0.08086181210227616, 'xgb_max_depth': 5, 'xgb_subsample': 0.7322799012505111, 'xgb_colsample_bytree': 0.662655113151194, 'log_reg_C': 0.31983305864468126, 'log_reg_solver': 'liblinear'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:49,434] Trial 4 finished with value: 0.9465391149758562 and parameters: {'rf_n_estimators': 198, 'rf_max_depth': 20, 'rf_min_samples_split': 9, 'xgb_n_estimators': 163, 'xgb_learning_rate': 0.05787150953759566, 'xgb_max_depth': 4, 'xgb_subsample': 0.8282501923738756, 'xgb_colsample_bytree': 0.8961939893517705, 'log_reg_C': 0.9275820900215785, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:50,726] Trial 5 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 200, 'rf_max_depth': 14, 'rf_min_samples_split': 16, 'xgb_n_estimators': 69, 'xgb_learning_rate': 0.16750990892579276, 'xgb_max_depth': 3, 'xgb_subsample': 0.62115453400898, 'xgb_colsample_bytree': 0.8198739585563658, 'log_reg_C': 0.0138975372596006, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:53,365] Trial 6 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 147, 'rf_max_depth': 15, 'rf_min_samples_split': 7, 'xgb_n_estimators': 281, 'xgb_learning_rate': 0.015305775134469667, 'xgb_max_depth': 8, 'xgb_subsample': 0.9562379330375597, 'xgb_colsample_bytree': 0.9475638967609769, 'log_reg_C': 1.5911133029769813, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:54,535] Trial 7 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 80, 'rf_max_depth': 3, 'rf_min_samples_split': 19, 'xgb_n_estimators': 162, 'xgb_learning_rate': 0.07298859056335374, 'xgb_max_depth': 5, 'xgb_subsample': 0.8035613021081957, 'xgb_colsample_bytree': 0.864934111056351, 'log_reg_C': 0.5664748670271927, 'log_reg_solver': 'saga'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:55,779] Trial 8 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 173, 'rf_max_depth': 20, 'rf_min_samples_split': 6, 'xgb_n_estimators': 98, 'xgb_learning_rate': 0.01901543788711685, 'xgb_max_depth': 4, 'xgb_subsample': 0.6420656226572136, 'xgb_colsample_bytree': 0.786202463516002, 'log_reg_C': 1.0728820598986735, 'log_reg_solver': 'lbfgs'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:57,616] Trial 9 finished with value: 0.9465391149758562 and parameters: {'rf_n_estimators': 248, 'rf_max_depth': 4, 'rf_min_samples_split': 15, 'xgb_n_estimators': 127, 'xgb_learning_rate': 0.18301557366004337, 'xgb_max_depth': 8, 'xgb_subsample': 0.9807666123160839, 'xgb_colsample_bytree': 0.6857565464994042, 'log_reg_C': 9.155894698786637, 'log_reg_solver': 'saga'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:42:59,900] Trial 10 finished with value: 0.9523411563829395 and parameters: {'rf_n_estimators': 127, 'rf_max_depth': 8, 'rf_min_samples_split': 2, 'xgb_n_estimators': 214, 'xgb_learning_rate': 0.028151271896630414, 'xgb_max_depth': 10, 'xgb_subsample': 0.8966175655853781, 'xgb_colsample_bytree': 0.9807881540825996, 'log_reg_C': 0.0313008174315488, 'log_reg_solver': 'liblinear'}. Best is trial 0 with value: 0.9524198078256308.\n",
      "[I 2025-04-09 13:43:02,254] Trial 11 finished with value: 0.9542609835857856 and parameters: {'rf_n_estimators': 124, 'rf_max_depth': 8, 'rf_min_samples_split': 2, 'xgb_n_estimators': 227, 'xgb_learning_rate': 0.02411785491398546, 'xgb_max_depth': 10, 'xgb_subsample': 0.9173229807998943, 'xgb_colsample_bytree': 0.9809914380784289, 'log_reg_C': 0.03053110065048662, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:04,425] Trial 12 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 121, 'rf_max_depth': 7, 'rf_min_samples_split': 2, 'xgb_n_estimators': 267, 'xgb_learning_rate': 0.01013816763964868, 'xgb_max_depth': 10, 'xgb_subsample': 0.8966441531392955, 'xgb_colsample_bytree': 0.9149215088077152, 'log_reg_C': 0.06942693041135774, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:06,511] Trial 13 finished with value: 0.9524029599963523 and parameters: {'rf_n_estimators': 97, 'rf_max_depth': 8, 'rf_min_samples_split': 11, 'xgb_n_estimators': 226, 'xgb_learning_rate': 0.03004666582936152, 'xgb_max_depth': 7, 'xgb_subsample': 0.9055530850714131, 'xgb_colsample_bytree': 0.7311873297326864, 'log_reg_C': 0.08675651931194345, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:08,816] Trial 14 finished with value: 0.9504435986030287 and parameters: {'rf_n_estimators': 163, 'rf_max_depth': 6, 'rf_min_samples_split': 7, 'xgb_n_estimators': 247, 'xgb_learning_rate': 0.03205684161302235, 'xgb_max_depth': 9, 'xgb_subsample': 0.9442606163430739, 'xgb_colsample_bytree': 0.9908739125648346, 'log_reg_C': 0.011135162025314633, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:11,212] Trial 15 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 260, 'rf_max_depth': 10, 'rf_min_samples_split': 11, 'xgb_n_estimators': 186, 'xgb_learning_rate': 0.011094804751378973, 'xgb_max_depth': 10, 'xgb_subsample': 0.8519327877316499, 'xgb_colsample_bytree': 0.7540261442222993, 'log_reg_C': 0.09829127028607382, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:12,618] Trial 16 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 230, 'rf_max_depth': 10, 'rf_min_samples_split': 4, 'xgb_n_estimators': 53, 'xgb_learning_rate': 0.01903213914249853, 'xgb_max_depth': 7, 'xgb_subsample': 0.8651748023784277, 'xgb_colsample_bytree': 0.9231656000503373, 'log_reg_C': 0.03400154301610198, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:14,474] Trial 17 finished with value: 0.9504604464323073 and parameters: {'rf_n_estimators': 141, 'rf_max_depth': 5, 'rf_min_samples_split': 2, 'xgb_n_estimators': 134, 'xgb_learning_rate': 0.03750420371883635, 'xgb_max_depth': 9, 'xgb_subsample': 0.9995338107422203, 'xgb_colsample_bytree': 0.6243782233325947, 'log_reg_C': 0.1597062365101021, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:17,621] Trial 18 finished with value: 0.9504435986030287 and parameters: {'rf_n_estimators': 101, 'rf_max_depth': 9, 'rf_min_samples_split': 5, 'xgb_n_estimators': 299, 'xgb_learning_rate': 0.016097443088549768, 'xgb_max_depth': 8, 'xgb_subsample': 0.9343102990982347, 'xgb_colsample_bytree': 0.8719573929818037, 'log_reg_C': 0.029198791003168106, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:20,051] Trial 19 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 187, 'rf_max_depth': 16, 'rf_min_samples_split': 9, 'xgb_n_estimators': 181, 'xgb_learning_rate': 0.023520139070476266, 'xgb_max_depth': 6, 'xgb_subsample': 0.7461024250118421, 'xgb_colsample_bytree': 0.9598118985811714, 'log_reg_C': 0.18944739488631873, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:22,013] Trial 20 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 57, 'rf_max_depth': 18, 'rf_min_samples_split': 9, 'xgb_n_estimators': 198, 'xgb_learning_rate': 0.0135358647797468, 'xgb_max_depth': 9, 'xgb_subsample': 0.9219188730917791, 'xgb_colsample_bytree': 0.7605896983514627, 'log_reg_C': 0.021036894457096313, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:24,999] Trial 21 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 123, 'rf_max_depth': 7, 'rf_min_samples_split': 2, 'xgb_n_estimators': 256, 'xgb_learning_rate': 0.010867550512013213, 'xgb_max_depth': 10, 'xgb_subsample': 0.876770600489498, 'xgb_colsample_bytree': 0.8999668995355191, 'log_reg_C': 0.060713018694029855, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:28,466] Trial 22 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 115, 'rf_max_depth': 6, 'rf_min_samples_split': 2, 'xgb_n_estimators': 263, 'xgb_learning_rate': 0.011057188761996791, 'xgb_max_depth': 10, 'xgb_subsample': 0.9628980086235172, 'xgb_colsample_bytree': 0.9391551255576814, 'log_reg_C': 0.058430017599614435, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:31,270] Trial 23 finished with value: 0.9523411563829395 and parameters: {'rf_n_estimators': 157, 'rf_max_depth': 8, 'rf_min_samples_split': 4, 'xgb_n_estimators': 237, 'xgb_learning_rate': 0.03934143308782352, 'xgb_max_depth': 10, 'xgb_subsample': 0.8333107281147506, 'xgb_colsample_bytree': 0.9111231839447318, 'log_reg_C': 0.047095901133404595, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:34,716] Trial 24 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 226, 'rf_max_depth': 10, 'rf_min_samples_split': 6, 'xgb_n_estimators': 273, 'xgb_learning_rate': 0.021566828396496308, 'xgb_max_depth': 9, 'xgb_subsample': 0.9038310924946438, 'xgb_colsample_bytree': 0.8537192763697432, 'log_reg_C': 0.01795976252247712, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:37,947] Trial 25 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 135, 'rf_max_depth': 6, 'rf_min_samples_split': 3, 'xgb_n_estimators': 288, 'xgb_learning_rate': 0.014113785537740274, 'xgb_max_depth': 8, 'xgb_subsample': 0.999027275263951, 'xgb_colsample_bytree': 0.9716445624277076, 'log_reg_C': 0.1369376507679683, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:40,192] Trial 26 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 86, 'rf_max_depth': 12, 'rf_min_samples_split': 6, 'xgb_n_estimators': 206, 'xgb_learning_rate': 0.010537521690271912, 'xgb_max_depth': 10, 'xgb_subsample': 0.8784002199339324, 'xgb_colsample_bytree': 0.8860602300060623, 'log_reg_C': 0.2635943726617407, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:43,473] Trial 27 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 173, 'rf_max_depth': 4, 'rf_min_samples_split': 13, 'xgb_n_estimators': 235, 'xgb_learning_rate': 0.27319729903389156, 'xgb_max_depth': 9, 'xgb_subsample': 0.6968129312252527, 'xgb_colsample_bytree': 0.9981733215211824, 'log_reg_C': 0.08249136248437469, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:48,165] Trial 28 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 108, 'rf_max_depth': 9, 'rf_min_samples_split': 19, 'xgb_n_estimators': 149, 'xgb_learning_rate': 0.024363272887366748, 'xgb_max_depth': 7, 'xgb_subsample': 0.8033455840249987, 'xgb_colsample_bytree': 0.8301016659355956, 'log_reg_C': 0.022695310338980836, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:43:53,378] Trial 29 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 295, 'rf_max_depth': 11, 'rf_min_samples_split': 8, 'xgb_n_estimators': 104, 'xgb_learning_rate': 0.04576537997508525, 'xgb_max_depth': 10, 'xgb_subsample': 0.9290726030250915, 'xgb_colsample_bytree': 0.9428371337476629, 'log_reg_C': 0.010033911021883649, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:01,442] Trial 30 finished with value: 0.9504604464323073 and parameters: {'rf_n_estimators': 76, 'rf_max_depth': 7, 'rf_min_samples_split': 3, 'xgb_n_estimators': 262, 'xgb_learning_rate': 0.016817202158583625, 'xgb_max_depth': 6, 'xgb_subsample': 0.9683569610921443, 'xgb_colsample_bytree': 0.9235819517227689, 'log_reg_C': 0.04535627990438538, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:06,137] Trial 31 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 265, 'rf_max_depth': 10, 'rf_min_samples_split': 11, 'xgb_n_estimators': 186, 'xgb_learning_rate': 0.012485836526706533, 'xgb_max_depth': 10, 'xgb_subsample': 0.8510633492967763, 'xgb_colsample_bytree': 0.7860193370856366, 'log_reg_C': 0.10348971959574087, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:08,684] Trial 32 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 248, 'rf_max_depth': 9, 'rf_min_samples_split': 17, 'xgb_n_estimators': 196, 'xgb_learning_rate': 0.010351826557172625, 'xgb_max_depth': 9, 'xgb_subsample': 0.8454399507043375, 'xgb_colsample_bytree': 0.73300083607086, 'log_reg_C': 0.07598403056072549, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:11,614] Trial 33 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 277, 'rf_max_depth': 13, 'rf_min_samples_split': 13, 'xgb_n_estimators': 220, 'xgb_learning_rate': 0.013845498322783072, 'xgb_max_depth': 10, 'xgb_subsample': 0.8901773920882302, 'xgb_colsample_bytree': 0.7516111504652667, 'log_reg_C': 0.12016252367451073, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:13,723] Trial 34 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 218, 'rf_max_depth': 11, 'rf_min_samples_split': 3, 'xgb_n_estimators': 157, 'xgb_learning_rate': 0.02007304836584599, 'xgb_max_depth': 9, 'xgb_subsample': 0.7661110767906222, 'xgb_colsample_bytree': 0.8086609195709527, 'log_reg_C': 0.24977289990704563, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:15,941] Trial 35 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 191, 'rf_max_depth': 12, 'rf_min_samples_split': 5, 'xgb_n_estimators': 174, 'xgb_learning_rate': 0.012414125026704836, 'xgb_max_depth': 10, 'xgb_subsample': 0.8212388773598277, 'xgb_colsample_bytree': 0.8465029411977301, 'log_reg_C': 0.035231050959016644, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:20,275] Trial 36 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 300, 'rf_max_depth': 7, 'rf_min_samples_split': 5, 'xgb_n_estimators': 247, 'xgb_learning_rate': 0.016792997291640677, 'xgb_max_depth': 8, 'xgb_subsample': 0.9248512284202646, 'xgb_colsample_bytree': 0.6865612432069057, 'log_reg_C': 4.864295119486604, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:21,961] Trial 37 finished with value: 0.9465391149758562 and parameters: {'rf_n_estimators': 157, 'rf_max_depth': 13, 'rf_min_samples_split': 11, 'xgb_n_estimators': 145, 'xgb_learning_rate': 0.11034482495461768, 'xgb_max_depth': 3, 'xgb_subsample': 0.8605415567077819, 'xgb_colsample_bytree': 0.8325659299154895, 'log_reg_C': 0.5394123907431289, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:23,615] Trial 38 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 241, 'rf_max_depth': 5, 'rf_min_samples_split': 3, 'xgb_n_estimators': 75, 'xgb_learning_rate': 0.010135029406501295, 'xgb_max_depth': 9, 'xgb_subsample': 0.7838150647125514, 'xgb_colsample_bytree': 0.8794878937943325, 'log_reg_C': 0.016691197422952313, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:25,429] Trial 39 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 265, 'rf_max_depth': 9, 'rf_min_samples_split': 20, 'xgb_n_estimators': 92, 'xgb_learning_rate': 0.02563176713292319, 'xgb_max_depth': 10, 'xgb_subsample': 0.7065488155399763, 'xgb_colsample_bytree': 0.9567632475372819, 'log_reg_C': 0.04421145611428081, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:28,177] Trial 40 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 210, 'rf_max_depth': 10, 'rf_min_samples_split': 17, 'xgb_n_estimators': 229, 'xgb_learning_rate': 0.01746906875966048, 'xgb_max_depth': 8, 'xgb_subsample': 0.9084140925441205, 'xgb_colsample_bytree': 0.7874240749588107, 'log_reg_C': 0.3809930439737176, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:29,494] Trial 41 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 235, 'rf_max_depth': 11, 'rf_min_samples_split': 4, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.020455260576059866, 'xgb_max_depth': 5, 'xgb_subsample': 0.8646586362768911, 'xgb_colsample_bytree': 0.9266736863290774, 'log_reg_C': 0.02650451179943385, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:30,796] Trial 42 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 203, 'rf_max_depth': 8, 'rf_min_samples_split': 4, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.012381285049948643, 'xgb_max_depth': 7, 'xgb_subsample': 0.9493679612555397, 'xgb_colsample_bytree': 0.9018122470848527, 'log_reg_C': 0.037789428515119454, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:32,310] Trial 43 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 255, 'rf_max_depth': 10, 'rf_min_samples_split': 2, 'xgb_n_estimators': 64, 'xgb_learning_rate': 0.014399196614041921, 'xgb_max_depth': 4, 'xgb_subsample': 0.8848742891926713, 'xgb_colsample_bytree': 0.9693132149008914, 'log_reg_C': 0.06116365217232539, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:34,166] Trial 44 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 231, 'rf_max_depth': 14, 'rf_min_samples_split': 5, 'xgb_n_estimators': 110, 'xgb_learning_rate': 0.019368144447576733, 'xgb_max_depth': 9, 'xgb_subsample': 0.8248407291766712, 'xgb_colsample_bytree': 0.9238814453593882, 'log_reg_C': 0.014293358333049785, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:35,897] Trial 45 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 221, 'rf_max_depth': 7, 'rf_min_samples_split': 3, 'xgb_n_estimators': 90, 'xgb_learning_rate': 0.06592869673971673, 'xgb_max_depth': 5, 'xgb_subsample': 0.9090763944768543, 'xgb_colsample_bytree': 0.8543815473019236, 'log_reg_C': 0.028167225701218174, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:37,415] Trial 46 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 188, 'rf_max_depth': 8, 'rf_min_samples_split': 7, 'xgb_n_estimators': 75, 'xgb_learning_rate': 0.03385486436603112, 'xgb_max_depth': 10, 'xgb_subsample': 0.9781170108236582, 'xgb_colsample_bytree': 0.8871001211316992, 'log_reg_C': 0.09606924552947964, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:39,005] Trial 47 finished with value: 0.95050238562519 and parameters: {'rf_n_estimators': 144, 'rf_max_depth': 9, 'rf_min_samples_split': 8, 'xgb_n_estimators': 116, 'xgb_learning_rate': 0.015298395302048138, 'xgb_max_depth': 6, 'xgb_subsample': 0.8678348265979394, 'xgb_colsample_bytree': 0.9807931793221519, 'log_reg_C': 0.16867249248218175, 'log_reg_solver': 'lbfgs'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:42,005] Trial 48 finished with value: 0.9524198078256308 and parameters: {'rf_n_estimators': 282, 'rf_max_depth': 12, 'rf_min_samples_split': 13, 'xgb_n_estimators': 57, 'xgb_learning_rate': 0.01207319645850501, 'xgb_max_depth': 9, 'xgb_subsample': 0.9415179948621077, 'xgb_colsample_bytree': 0.7146434051115657, 'log_reg_C': 0.05867685889365731, 'log_reg_solver': 'liblinear'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:47,473] Trial 49 finished with value: 0.9485430242318662 and parameters: {'rf_n_estimators': 209, 'rf_max_depth': 5, 'rf_min_samples_split': 10, 'xgb_n_estimators': 130, 'xgb_learning_rate': 0.02808093011907194, 'xgb_max_depth': 10, 'xgb_subsample': 0.8461895806692906, 'xgb_colsample_bytree': 0.9447714036036781, 'log_reg_C': 2.1822394894359984, 'log_reg_solver': 'saga'}. Best is trial 11 with value: 0.9542609835857856.\n",
      "[I 2025-04-09 13:44:47,475] A new study created in memory with name: no-name-48977e24-1c1f-49db-9bc7-28568b26ab9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial for Ensemble Model:\n",
      "  Value: 0.9543\n",
      "  Params: \n",
      "    rf_n_estimators: 124\n",
      "    rf_max_depth: 8\n",
      "    rf_min_samples_split: 2\n",
      "    xgb_n_estimators: 227\n",
      "    xgb_learning_rate: 0.02411785491398546\n",
      "    xgb_max_depth: 10\n",
      "    xgb_subsample: 0.9173229807998943\n",
      "    xgb_colsample_bytree: 0.9809914380784289\n",
      "    log_reg_C: 0.03053110065048662\n",
      "    log_reg_solver: liblinear\n",
      "\n",
      "Optimizing Stacking Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-09 13:45:16,656] Trial 0 finished with value: 0.942759737669679 and parameters: {'rf_n_estimators': 209, 'rf_max_depth': 6, 'rf_min_samples_split': 10, 'xgb_n_estimators': 191, 'xgb_learning_rate': 0.02961989390589191, 'xgb_max_depth': 4, 'xgb_subsample': 0.9856293225624563, 'xgb_colsample_bytree': 0.6674659637252496, 'log_reg_C': 0.07368528440631428, 'log_reg_solver': 'saga', 'final_rf_n_estimators': 217, 'final_rf_max_depth': 17}. Best is trial 0 with value: 0.942759737669679.\n",
      "[I 2025-04-09 13:45:40,240] Trial 1 finished with value: 0.9462833429306396 and parameters: {'rf_n_estimators': 262, 'rf_max_depth': 13, 'rf_min_samples_split': 19, 'xgb_n_estimators': 215, 'xgb_learning_rate': 0.11680441378112139, 'xgb_max_depth': 5, 'xgb_subsample': 0.6074470882013117, 'xgb_colsample_bytree': 0.8467693021750542, 'log_reg_C': 7.336936649399507, 'log_reg_solver': 'lbfgs', 'final_rf_n_estimators': 175, 'final_rf_max_depth': 12}. Best is trial 1 with value: 0.9462833429306396.\n",
      "[I 2025-04-09 13:46:05,261] Trial 2 finished with value: 0.9387346898120642 and parameters: {'rf_n_estimators': 218, 'rf_max_depth': 20, 'rf_min_samples_split': 20, 'xgb_n_estimators': 217, 'xgb_learning_rate': 0.012058809069989634, 'xgb_max_depth': 4, 'xgb_subsample': 0.9078071389018213, 'xgb_colsample_bytree': 0.721441029539663, 'log_reg_C': 7.400821756727694, 'log_reg_solver': 'saga', 'final_rf_n_estimators': 275, 'final_rf_max_depth': 10}. Best is trial 1 with value: 0.9462833429306396.\n",
      "[I 2025-04-09 13:46:22,138] Trial 3 finished with value: 0.9446597912684009 and parameters: {'rf_n_estimators': 88, 'rf_max_depth': 15, 'rf_min_samples_split': 13, 'xgb_n_estimators': 167, 'xgb_learning_rate': 0.06229934795530329, 'xgb_max_depth': 7, 'xgb_subsample': 0.717101856573544, 'xgb_colsample_bytree': 0.6560271857744111, 'log_reg_C': 0.021608030831420717, 'log_reg_solver': 'liblinear', 'final_rf_n_estimators': 256, 'final_rf_max_depth': 8}. Best is trial 1 with value: 0.9462833429306396.\n",
      "[I 2025-04-09 13:46:37,573] Trial 4 finished with value: 0.9467475412880908 and parameters: {'rf_n_estimators': 249, 'rf_max_depth': 20, 'rf_min_samples_split': 14, 'xgb_n_estimators': 89, 'xgb_learning_rate': 0.17422719504339462, 'xgb_max_depth': 10, 'xgb_subsample': 0.7125787491648509, 'xgb_colsample_bytree': 0.852596971487401, 'log_reg_C': 7.513687018872293, 'log_reg_solver': 'saga', 'final_rf_n_estimators': 267, 'final_rf_max_depth': 3}. Best is trial 4 with value: 0.9467475412880908.\n",
      "[I 2025-04-09 13:46:52,324] Trial 5 finished with value: 0.9486127730616886 and parameters: {'rf_n_estimators': 175, 'rf_max_depth': 9, 'rf_min_samples_split': 10, 'xgb_n_estimators': 90, 'xgb_learning_rate': 0.07932479582164688, 'xgb_max_depth': 8, 'xgb_subsample': 0.727654598555995, 'xgb_colsample_bytree': 0.6434094665241331, 'log_reg_C': 1.2464295764828695, 'log_reg_solver': 'lbfgs', 'final_rf_n_estimators': 135, 'final_rf_max_depth': 3}. Best is trial 5 with value: 0.9486127730616886.\n",
      "[I 2025-04-09 13:47:01,426] Trial 6 finished with value: 0.944376157002802 and parameters: {'rf_n_estimators': 104, 'rf_max_depth': 18, 'rf_min_samples_split': 5, 'xgb_n_estimators': 77, 'xgb_learning_rate': 0.2831013924291632, 'xgb_max_depth': 9, 'xgb_subsample': 0.8069000915487967, 'xgb_colsample_bytree': 0.9308949890593117, 'log_reg_C': 0.7611220885611175, 'log_reg_solver': 'lbfgs', 'final_rf_n_estimators': 55, 'final_rf_max_depth': 11}. Best is trial 5 with value: 0.9486127730616886.\n",
      "[I 2025-04-09 13:47:10,959] Trial 7 finished with value: 0.944478032537728 and parameters: {'rf_n_estimators': 110, 'rf_max_depth': 10, 'rf_min_samples_split': 19, 'xgb_n_estimators': 91, 'xgb_learning_rate': 0.2595553517924945, 'xgb_max_depth': 10, 'xgb_subsample': 0.6759462819444955, 'xgb_colsample_bytree': 0.9041830821374709, 'log_reg_C': 0.8846734010110174, 'log_reg_solver': 'lbfgs', 'final_rf_n_estimators': 201, 'final_rf_max_depth': 13}. Best is trial 5 with value: 0.9486127730616886.\n",
      "[I 2025-04-09 13:47:28,806] Trial 8 finished with value: 0.948451140093046 and parameters: {'rf_n_estimators': 152, 'rf_max_depth': 10, 'rf_min_samples_split': 2, 'xgb_n_estimators': 210, 'xgb_learning_rate': 0.21807173682562134, 'xgb_max_depth': 8, 'xgb_subsample': 0.9203820949012465, 'xgb_colsample_bytree': 0.831961849049543, 'log_reg_C': 0.18378011119376791, 'log_reg_solver': 'liblinear', 'final_rf_n_estimators': 285, 'final_rf_max_depth': 6}. Best is trial 5 with value: 0.9486127730616886.\n",
      "[I 2025-04-09 13:47:46,710] Trial 9 finished with value: 0.9425170196943624 and parameters: {'rf_n_estimators': 165, 'rf_max_depth': 5, 'rf_min_samples_split': 19, 'xgb_n_estimators': 274, 'xgb_learning_rate': 0.2880953287980381, 'xgb_max_depth': 9, 'xgb_subsample': 0.7129443324678669, 'xgb_colsample_bytree': 0.8668593077383164, 'log_reg_C': 8.149509562666932, 'log_reg_solver': 'liblinear', 'final_rf_n_estimators': 245, 'final_rf_max_depth': 12}. Best is trial 5 with value: 0.9486127730616886.\n",
      "[I 2025-04-09 13:47:58,138] Trial 10 finished with value: 0.9407019381971023 and parameters: {'rf_n_estimators': 58, 'rf_max_depth': 3, 'rf_min_samples_split': 8, 'xgb_n_estimators': 138, 'xgb_learning_rate': 0.0605196672079705, 'xgb_max_depth': 6, 'xgb_subsample': 0.798434276529799, 'xgb_colsample_bytree': 0.9966700722225946, 'log_reg_C': 1.4294071862193447, 'log_reg_solver': 'lbfgs', 'final_rf_n_estimators': 131, 'final_rf_max_depth': 20}. Best is trial 5 with value: 0.9486127730616886.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\mlpr data\\Glioblastoma-ML-model\\stackAndModel\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert target column to numeric\n",
    "df[\"Survival_from_surgery_days_UPDATED\"] = pd.to_numeric(df[\"Survival_from_surgery_days_UPDATED\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target variable is NaN\n",
    "df = df.dropna(subset=[\"Survival_from_surgery_days_UPDATED\"])\n",
    "\n",
    "# Percentile-Based Binning\n",
    "percentiles = np.percentile(df[\"Survival_from_surgery_days_UPDATED\"], [25, 50, 75])\n",
    "bins = [0, percentiles[0], percentiles[1], percentiles[2], np.inf]\n",
    "labels = [0, 1, 2, 3]\n",
    "\n",
    "df[\"Survival_Category\"] = pd.cut(df[\"Survival_from_surgery_days_UPDATED\"], bins=bins, labels=labels)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"PatientID\", \"Survival_from_surgery_days_UPDATED\", \"Survival_Category\"])\n",
    "y = df[\"Survival_Category\"]\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(X.median())\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA for dimensionality reduction\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)  # Adjust components based on the number of classes - 1\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train-test split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance Classes with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the objective function for Optuna (Ensemble Model)\n",
    "def objective_ensemble(trial):\n",
    "    # Hyperparameters for Random Forest\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 300)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 3, 20)\n",
    "    rf_min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 20)\n",
    "    \n",
    "    # Hyperparameters for XGBoost\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
    "    xgb_subsample = trial.suggest_float('xgb_subsample', 0.6, 1.0)\n",
    "    xgb_colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0)\n",
    "    \n",
    "    # Hyperparameters for Logistic Regression\n",
    "    log_reg_C = trial.suggest_float('log_reg_C', 0.01, 10.0, log=True)\n",
    "    log_reg_solver = trial.suggest_categorical('log_reg_solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "    \n",
    "    # Define models with the suggested hyperparameters\n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        n_estimators=xgb_n_estimators,\n",
    "        learning_rate=xgb_learning_rate,\n",
    "        max_depth=xgb_max_depth,\n",
    "        subsample=xgb_subsample,\n",
    "        colsample_bytree=xgb_colsample_bytree,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    log_reg = LogisticRegression(\n",
    "        C=log_reg_C,\n",
    "        solver=log_reg_solver,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    # Create the ensemble model\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg), (\"LDA\", lda_clf)],\n",
    "        voting=\"soft\"\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        ensemble_model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = ensemble_model.predict(X_val_fold)\n",
    "        score = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Define the objective function for Optuna (Stacking Model)\n",
    "def objective_stacking(trial):\n",
    "    # Hyperparameters for Random Forest\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 300)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 3, 20)\n",
    "    rf_min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 20)\n",
    "    \n",
    "    # Hyperparameters for XGBoost\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
    "    xgb_subsample = trial.suggest_float('xgb_subsample', 0.6, 1.0)\n",
    "    xgb_colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0)\n",
    "    \n",
    "    # Hyperparameters for Logistic Regression\n",
    "    log_reg_C = trial.suggest_float('log_reg_C', 0.01, 10.0, log=True)\n",
    "    log_reg_solver = trial.suggest_categorical('log_reg_solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "    \n",
    "    # Hyperparameters for final estimator (Random Forest)\n",
    "    final_rf_n_estimators = trial.suggest_int('final_rf_n_estimators', 50, 300)\n",
    "    final_rf_max_depth = trial.suggest_int('final_rf_max_depth', 3, 20)\n",
    "    \n",
    "    # Define models with the suggested hyperparameters\n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        n_estimators=xgb_n_estimators,\n",
    "        learning_rate=xgb_learning_rate,\n",
    "        max_depth=xgb_max_depth,\n",
    "        subsample=xgb_subsample,\n",
    "        colsample_bytree=xgb_colsample_bytree,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    log_reg = LogisticRegression(\n",
    "        C=log_reg_C,\n",
    "        solver=log_reg_solver,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    final_estimator = RandomForestClassifier(\n",
    "        n_estimators=final_rf_n_estimators,\n",
    "        max_depth=final_rf_max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create the stacking model\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[(\"RandomForest\", rf_clf), (\"XGBoost\", xgb_clf), (\"LogReg\", log_reg), (\"LDA\", lda_clf)],\n",
    "        final_estimator=final_estimator,\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        stacking_clf.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = stacking_clf.predict(X_val_fold)\n",
    "        score = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna optimization for Ensemble Model\n",
    "print(\"Optimizing Ensemble Model...\")\n",
    "study_ensemble = optuna.create_study(direction='maximize')\n",
    "study_ensemble.optimize(objective_ensemble, n_trials=50)  # Adjust n_trials as needed\n",
    "\n",
    "print(\"Best trial for Ensemble Model:\")\n",
    "print(f\"  Value: {study_ensemble.best_value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in study_ensemble.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Run Optuna optimization for Stacking Model\n",
    "print(\"\\nOptimizing Stacking Model...\")\n",
    "study_stacking = optuna.create_study(direction='maximize')\n",
    "study_stacking.optimize(objective_stacking, n_trials=50)  # Adjust n_trials as needed\n",
    "\n",
    "print(\"Best trial for Stacking Model:\")\n",
    "print(f\"  Value: {study_stacking.best_value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in study_stacking.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Create and train the best Ensemble Model\n",
    "best_rf_ensemble = RandomForestClassifier(\n",
    "    n_estimators=study_ensemble.best_params['rf_n_estimators'],\n",
    "    max_depth=study_ensemble.best_params['rf_max_depth'],\n",
    "    min_samples_split=study_ensemble.best_params['rf_min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_xgb_ensemble = xgb.XGBClassifier(\n",
    "    n_estimators=study_ensemble.best_params['xgb_n_estimators'],\n",
    "    learning_rate=study_ensemble.best_params['xgb_learning_rate'],\n",
    "    max_depth=study_ensemble.best_params['xgb_max_depth'],\n",
    "    subsample=study_ensemble.best_params['xgb_subsample'],\n",
    "    colsample_bytree=study_ensemble.best_params['xgb_colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_log_reg_ensemble = LogisticRegression(\n",
    "    C=study_ensemble.best_params['log_reg_C'],\n",
    "    solver=study_ensemble.best_params['log_reg_solver'],\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_lda_ensemble = LinearDiscriminantAnalysis()\n",
    "\n",
    "best_ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomForest\", best_rf_ensemble), \n",
    "        (\"XGBoost\", best_xgb_ensemble), \n",
    "        (\"LogReg\", best_log_reg_ensemble), \n",
    "        (\"LDA\", best_lda_ensemble)\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "# Create and train the best Stacking Model\n",
    "best_rf_stacking = RandomForestClassifier(\n",
    "    n_estimators=study_stacking.best_params['rf_n_estimators'],\n",
    "    max_depth=study_stacking.best_params['rf_max_depth'],\n",
    "    min_samples_split=study_stacking.best_params['rf_min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_xgb_stacking = xgb.XGBClassifier(\n",
    "    n_estimators=study_stacking.best_params['xgb_n_estimators'],\n",
    "    learning_rate=study_stacking.best_params['xgb_learning_rate'],\n",
    "    max_depth=study_stacking.best_params['xgb_max_depth'],\n",
    "    subsample=study_stacking.best_params['xgb_subsample'],\n",
    "    colsample_bytree=study_stacking.best_params['xgb_colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_log_reg_stacking = LogisticRegression(\n",
    "    C=study_stacking.best_params['log_reg_C'],\n",
    "    solver=study_stacking.best_params['log_reg_solver'],\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_lda_stacking = LinearDiscriminantAnalysis()\n",
    "\n",
    "best_final_estimator = RandomForestClassifier(\n",
    "    n_estimators=study_stacking.best_params['final_rf_n_estimators'],\n",
    "    max_depth=study_stacking.best_params['final_rf_max_depth'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomForest\", best_rf_stacking), \n",
    "        (\"XGBoost\", best_xgb_stacking), \n",
    "        (\"LogReg\", best_log_reg_stacking), \n",
    "        (\"LDA\", best_lda_stacking)\n",
    "    ],\n",
    "    final_estimator=best_final_estimator,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the best models\n",
    "print(\"\\nTraining best models...\")\n",
    "best_ensemble_model.fit(X_train, y_train)\n",
    "best_stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best models\n",
    "y_pred_best_ensemble = best_ensemble_model.predict(X_test)\n",
    "y_pred_best_stacking = best_stacking_model.predict(X_test)\n",
    "\n",
    "accuracy_best_ensemble = accuracy_score(y_test, y_pred_best_ensemble)\n",
    "accuracy_best_stacking = accuracy_score(y_test, y_pred_best_stacking)\n",
    "\n",
    "report_best_ensemble = classification_report(y_test, y_pred_best_ensemble)\n",
    "report_best_stacking = classification_report(y_test, y_pred_best_stacking)\n",
    "\n",
    "print(f\"\\nBest Ensemble Model Accuracy with LDA: {accuracy_best_ensemble:.4f}\")\n",
    "print(\"Classification Report:\\n\", report_best_ensemble)\n",
    "\n",
    "print(f\"\\nBest Stacking Model Accuracy with LDA: {accuracy_best_stacking:.4f}\")\n",
    "print(\"Classification Report:\\n\", report_best_stacking)\n",
    "\n",
    "# Visualize Optuna results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_optimization_history(study_ensemble)\n",
    "plt.title(\"Optimization History (Ensemble)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_param_importances(study_ensemble)\n",
    "plt.title(\"Parameter Importances (Ensemble)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_optuna_results.png')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_optimization_history(study_stacking)\n",
    "plt.title(\"Optimization History (Stacking)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_param_importances(study_stacking)\n",
    "plt.title(\"Parameter Importances (Stacking)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('stacking_optuna_results.png')\n",
    "\n",
    "print(\"\\nOptimization plots saved as 'ensemble_optuna_results.png' and 'stacking_optuna_results.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
